{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "In machine learning, models are trained to predict unknown labels for new data based on correlations between known labels and features found in the training data. Depending on the algorithm used, you may need to specify hyperparameters to configure how the model is trained. For example, the logistic regression algorithm uses a regularization rate hyperparameter to counteract overfitting; and deep learning techniques for convolutional neural networks (CNNs) use hyperparameters like learning rate to control how weights are adjusted during training, and batch size to determine how many data items are included in each training batch.\n",
    "\n",
    "<mark>**Note:** Machine Learning is an academic field with its own particular terminology. Data scientists refer to the values determined from the training features as parameters, so a different term is required for values that are used to configure training behavior but which are not derived from the training data - hence the term hyperparameter.</mark>\n",
    "\n",
    "**<h2>Tuning hyperparameters</h2>**\n",
    "\n",
    "\n",
    "<img src = \"images/08-01-hyperdrive.png\" />\n",
    "\n",
    "Hyperparameter tuning is accomplished by training the multiple models, using the same algorithm and training data but different hyperparameter values. The resulting model from each training run is then evaluated to determine the performance metric for which you want to optimize (for example, accuracy), and the best-performing model is selected.\n",
    "\n",
    "In Azure Machine Learning, you achieve this through an experiment that consists of a hyperdrive run, which initiates a child run for each hyperparameter combination to be tested. Each child run uses a training script with parameterized hyperparameter values to train a model, and logs the target performance metric achieved by the trained model.\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Learning objectives</h2>**\n",
    "\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Define a hyperparameter search space.\n",
    "- Configure hyperparameter sampling.\n",
    "- Select an early-termination policy.\n",
    "- Run a hyperparameter tuning experiment.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA-CEDrxJjw6"
   },
   "source": [
    "**<center><h1>Defining a search space</h1></center>**\n",
    "\n",
    "The set of hyperparameter values tried during hyperparameter tuning is known as the search space. The definition of the range of possible values that can be chosen depends on the type of hyperparameter.\n",
    "\n",
    "**<h2>Discrete hyperparameters</h2>**\n",
    "\n",
    "Some hyperparameters require discrete values - in other words, you must select the value from a particular set of possibilities. You can define a search space for a discrete parameter using a choice from a list of explicit values, which you can define as a Python list (`choice([10,20,30])`), a range (`choice(range(1,10))`), or an arbitrary set of comma-separated values (`choice(30,50,100)`)\n",
    "\n",
    "You can also select discrete values from any of the following discrete distributions:\n",
    "\n",
    "- qnormal\n",
    "- quniform\n",
    "- qlognormal\n",
    "- qloguniform\n",
    "\n",
    "**<h2>Continuous hyperparameters</h2>**\n",
    "\n",
    "Some hyperparameters are continuous - in other words you can use any value along a scale. To define a search space for these kinds of value, you can use any of the following distribution types:\n",
    "\n",
    "- normal\n",
    "- uniform\n",
    "- lognormal\n",
    "- loguniform\n",
    "\n",
    "\n",
    "**<h2>Defining a search space</h2>**\n",
    "\n",
    "To define a search space for hyperparameter tuning, create a dictionary with the appropriate parameter expression for each named hyperparameter. For example, the following search space indicates that the batch_size hyperparameter can have the value 16, 32, or 64, and the learning_rate hyperparameter can have any value from a normal distribution with a mean of 10 and a standard deviation of 3.\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "```\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DEMov1NKNtJ"
   },
   "source": [
    "**<center><h1>Configuring sampling</h1></center>**\n",
    "\n",
    "The specific values used in a hyperparameter tuning run depend on the type of sampling used.\n",
    "\n",
    "**<h2>Grid sampling</h2>**\n",
    "\n",
    "Grid sampling can only be employed when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space.\n",
    "\n",
    "For example, in the following code example, grid sampling is used to try every possible combination of discrete batch_size and learning_rate value:\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "              }\n",
    "\n",
    "param_sampling = GridParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "**<h2>Random sampling</h2>**\n",
    "\n",
    "Random sampling is used to randomly select a value for each hyperparameter, which can be a mix of discrete and continuous values as shown in the following code example:\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "\n",
    "param_sampling = RandomParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Bayesian sampling</h2>**\n",
    "\n",
    "Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection. The following code example shows how to configure Bayesian sampling:\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': uniform(0.05, 0.1)\n",
    "              }\n",
    "\n",
    "param_sampling = BayesianParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "You can only use Bayesian sampling with choice, uniform, and quniform parameter expressions, and you can't combine it with an early-termination policy.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG80fIfzKtyY"
   },
   "source": [
    "**<center><h2>Configuring early termination</h2></center>**\n",
    "\n",
    "With a sufficiently large hyperparameter search space, it could take many iterations (child runs) to try every possible combination. Typically, you set a maximum number of iterations, but this could still result in a large number of runs that don't result in a better model than a combination that has already been tried.\n",
    "\n",
    "To help prevent wasting time, you can set an early termination policy that abandons runs that are unlikely to produce a better result than previously completed runs. The policy is evaluated at an evaluation_interval you specify, based on each time the target performance metric is logged. You can also set a delay_evaluation parameter to avoid evaluating the policy until a minimum number of iterations have been completed.\n",
    "\n",
    "<mark>**Note:**Early termination is particularly useful for deep learning scenarios where a deep neural network (DNN) is trained iteratively over a number of epochs. The training script can report the target metric after each epoch, and if the run is significantly underperforming previous runs after the same number of intervals, it can be abandoned.</mark>\n",
    "\n",
    "\n",
    "**<h2>Bandit policy</h2>**\n",
    "\n",
    "You can use a bandit policy to stop a run if the target performance metric underperforms the best run so far by a specified margin.\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_amount = 0.2,\n",
    "                                        evaluation_interval=1,\n",
    "                                        delay_evaluation=5)\n",
    "```\n",
    "\n",
    "This example applies the policy for every iteration after the first five, and abandons runs where the reported target metric is 0.2 or more worse than the best performing run after the same number of intervals.\n",
    "\n",
    "You can also apply a bandit policy using a slack factor, which compares the performance metric as a ratio rather than an absolute value.\n",
    "\n",
    "**<h2>Median stopping policy</h2>**\n",
    "\n",
    "A median stopping policy abandons runs where the target performance metric is worse than the median of the running averages for all runs.\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,\n",
    "                                                delay_evaluation=5)\n",
    "                                    \n",
    "```\n",
    "\n",
    "\n",
    "**<h2>Truncation selection policy</h2>**\n",
    "\n",
    "A truncation selection policy cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X.\n",
    "\n",
    "```\n",
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "\n",
    "early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,\n",
    "                                                     evaluation_interval=1,\n",
    "                                                     delay_evaluation=5)\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2Cdu7hlLQQ0"
   },
   "source": [
    "**<center><h1>Running a hyperparameter tuning experiment</h1></center>**\n",
    "\n",
    "In Azure Machine Learning, you can tune hyperparameters by running a hyperdrive experiment.\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Creating a training script for hyperparameter tuning</h2>**\n",
    "\n",
    "To run a hyperdrive experiment, you need to create a training script just the way you would do for any other training experiment, except that your script **must**:\n",
    "\n",
    "- Include an argument for each hyperparameter you want to vary.\n",
    "- Log the target performance metric. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.\n",
    "For example, the following example script trains a logistic regression model using a **--regularization** argument to set the regularization rate hyperparameter, and logs the accuracy metric with the name **Accuracy**:\n",
    "\n",
    "```\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the training dataset\n",
    "data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels, and split for training/validatiom\n",
    "X = data[['feature1','feature2','feature3','feature4']].values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model with the reg hyperparameter\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate and log accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "<mark>**Note:** Note that in the Scikit-Learn LogisticRegression class, C is the inverse of the regularization rate; hence C=1/reg.</mark>\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Configuring and running a hyperdrive experiment</h2>**\n",
    "\n",
    "To prepare the hyperdrive experiment, you must use a **HyperDriveConfig** object to configure the experiment run, as shown in the following example code:\n",
    "\n",
    "```\n",
    "from azureml.core import Experiment\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "# Assumes ws, script_config and param_sampling are already defined\n",
    "\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config,\n",
    "                              hyperparameter_sampling=param_sampling,\n",
    "                              policy=None,\n",
    "                              primary_metric_name='Accuracy',\n",
    "                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                              max_total_runs=6,\n",
    "                              max_concurrent_runs=4)\n",
    "\n",
    "experiment = Experiment(workspace = ws, name = 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Monitoring and reviewing hyperdrive runs</h2>**\n",
    "\n",
    "You can monitor hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks **RunDetails** widget.\n",
    "\n",
    "The experiment will initiate a child run for each hyperparameter combination to be tried, and you can retrieve the logged metrics these runs using the following code:\n",
    "\n",
    "```\n",
    "for child_run in run.get_children():\n",
    "    print(child_run.id, child_run.get_metrics())\n",
    "```\n",
    "\n",
    "You can also list all runs in descending order of performance like this:\n",
    "\n",
    "```\n",
    "for child_run in hyperdrive_run.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "```\n",
    "\n",
    "To retrieve the best performing run, you can use the following code:\n",
    "\n",
    "```\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQeVc__dMOB-"
   },
   "source": [
    "**<center><h1>Exercise - Tune hyperparameters</h1></center>**\n",
    "\n",
    "Now it's your chance to run a hyperparameter tuning experiment.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Run a hyperparameter tuning experiment.\n",
    "- Review the results of the experiment.\n",
    "\n",
    "**<h2>Instructions </h2>**\n",
    "\n",
    "Follow these instructions to complete the exercise.\n",
    "\n",
    "1. If you do not already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com.\n",
    "2. View the exercise repo at https://aka.ms/mslearn-dp100.\n",
    "3. If you have not already done so, complete the **Create an Azure Machine Learning workspace** exercise to provision an Azure Machine Learning workspace, create a compute instance, and clone the required files.\n",
    "4. Complete the Tune hyperparameters exercise.\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayAa6TEBMjXT"
   },
   "source": [
    "**<center><h1>Knowledge check</h1></center>**\n",
    "\n",
    "1. You plan to use hyperparameter tuning to find optimal discrete values for a set of hyperparameters. You want to try every possible combination of a set of specified discrete values. Which kind of sampling should you use?\n",
    "\n",
    "- Random Sampling\n",
    "\n",
    "- Grid Sampling\n",
    "\n",
    "- Bayesian Sampling\n",
    "\n",
    "2. You are using hyper parameter tuning to train an optimal model based on a target metric named \"AUC\". What should you do in your training script?\n",
    "\n",
    "- Import the logging package and use a logging.info() statement to log the AUC\n",
    "\n",
    "- Include a print() statement to write the AUC value to the standard output stream\n",
    "\n",
    "- Get a reference to the Azure ML run context and use a run.log() statement to write the AUC value to the run log\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKpz-P7vMpAW"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "In this module, you learned how to:\n",
    "\n",
    "- Define a hyperparameter search space.\n",
    "- Configure hyperparameter sampling.\n",
    "- Select an early-termination policy.\n",
    "- Run a hyperparameter tuning experiment.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course3_Module8TunehyperparameterswithAzureMachineLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
