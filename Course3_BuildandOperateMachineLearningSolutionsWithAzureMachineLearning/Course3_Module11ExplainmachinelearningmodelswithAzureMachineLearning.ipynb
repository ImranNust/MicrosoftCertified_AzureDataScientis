{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "As machine learning becomes increasingly integral to decisions that affect health, safety, economic wellbeing, and other aspects of people's lives, it's important to be able to understand how models make predictions; and to be able to explain the rationale for machine learning based decisions.\n",
    "\n",
    "Explaining models is difficult because of the range of machine learning algorithm types and the nature of how machine learning works, but model interpretability has become a key element of helping to make model predictions explainable.\n",
    "\n",
    "**<h2>Learning objectives</h2>**\n",
    "\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Interpret global and local feature importance.\n",
    "- Use an explainer to interpret a model.\n",
    "- Create model explanations in a training experiment.\n",
    "- Visualize model explanations.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmIemVeBqz7o"
   },
   "source": [
    "**<center><h1>Feature importance</h1></center>**\n",
    "\n",
    "Model explainers use statistical techniques to calculate feature importance. This enables you to quantify the relative influence each feature in the training dataset has on label prediction. Explainers work by evaluating a test data set of feature cases and the labels the model predicts for them.\n",
    "\n",
    "**<h2>Global feature importance</h2>**\n",
    "\n",
    "Global feature importance quantifies the relative importance of each feature in the test dataset as a whole. It provides a general comparison of the extent to which each feature in the dataset influences prediction.\n",
    "\n",
    "For example, a binary classification model to predict loan default risk might be trained from features such as loan amount, income, marital status, and age to predict a label of 1 for loans that are likely to be repaid, and 0 for loans that have a significant risk of default (and therefore shouldn't be approved). An explainer might then use a sufficiently representative test dataset to produce the following global feature importance values:\n",
    "\n",
    "- income: 0.98\n",
    "- loan amount: 0.67\n",
    "- age: 0.54\n",
    "- marital status 0.32\n",
    "\n",
    "<img src=\"images/09-01-global-importance.png\" />\n",
    "\n",
    "\n",
    "It's clear from these values, that in respect to the overall predictions generated by the model for the test dataset, **income** is the most important feature for predicting whether or not a borrower will default on a loan, followed by the **loan amount**, then **age**, and finally **marital status**.\n",
    "\n",
    "**<h2>Local feature importance</h2>**\n",
    "\n",
    "Local feature importance measures the influence of each feature value for a specific individual prediction.\n",
    "\n",
    "For example, suppose Sam applies for a loan, which the machine learning model approves (by predicting that Sam won't default on the loan repayment). You could use an explainer to calculate the local feature importance for Sam's application to determine which factors influenced the prediction. You might get a result like this:\n",
    "\n",
    "<img src=\"images/image1.png\"/>\n",
    "\n",
    "Because this is a classification model, each feature gets a local importance value for each possible class, indicating the amount of support for that class based on the feature value. Since this is a binary classification model, there are only two possible classes (0 and 1). Each feature's support for one class results in correlatively negative level of support for the other.\n",
    "\n",
    "<img src=\"images/09-02-local-importance.png\"/>\n",
    "\n",
    "In Sam's case, the overall support for class 0 is -1.4, and the support for class 1 is correspondingly 1.4; so support for class 1 is higher than for class 0, and the loan is approved. The most important feature for a prediction of class 1 is loan amount, followed by income - these are the opposite order from their global feature importance values (which indicate that income is the most important factor for the data sample as a whole). There could be multiple reasons why local importance for an individual prediction varies from global importance for the overall dataset; for example, Sam might have a lower income than average, but the loan amount in this case might be unusually small.\n",
    "\n",
    "For a multi-class classification model, a local importance values for each possible class is calculated for every feature, with the total across all classes always being 0. For example, a model might predict the species of a penguin based on features like its bill length, bill width, flipper length, and weight. Suppose there are three species of penguin, so the model predicts one of three class labels (0, 1, or 2). For an individual prediction, the flipper length feature might have local importance values of 0.5 for class 0, 0.3 for class 1, and -0.8 for class 2 - indicating that the flipper length moderately supports a prediction of class 0, slightly supports a prediction of class 1, and strongly supports a prediction that this particular penguin is **not** class 2.\n",
    "\n",
    "For a regression model, there are no classes so the local importance values simply indicate the level of influence each feature has on the predicted scalar label.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2RDBafTq0PJ"
   },
   "source": [
    "**<center><h1>Using explainers</h1></center>**\n",
    "\n",
    "You can use the Azure Machine Learning SDK to create explainers for models, even if they were not trained using an Azure Machine Learning experiment.\n",
    "\n",
    "**<h2>Creating an explainer</h2>**\n",
    "\n",
    "To interpret a local model, you must install the azureml-interpret package and use it to create an explainer. There are multiple types of explainer, including:\n",
    "\n",
    "- **MimicExplainer** - An explainer that creates a global surrogate model that approximates your trained model and can be used to generate explanations. This explainable model must have the same kind of architecture as your trained model (for example, linear or tree-based).\n",
    "- **TabularExplainer** - An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate for your model architecture.\n",
    "- **PFIExplainer** - a Permutation Feature Importance explainer that analyzes feature importance by shuffling feature values and measuring the impact on prediction performance.\n",
    "\n",
    "The following code example shows how to create an instance of each of these explainer types for a hypothetical model named **loan_model**:\n",
    "```\n",
    "# MimicExplainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
    "\n",
    "mim_explainer = MimicExplainer(model=loan_model,\n",
    "                             initialization_examples=X_test,\n",
    "                             explainable_model = DecisionTreeExplainableModel,\n",
    "                             features=['loan_amount','income','age','marital_status'], \n",
    "                             classes=['reject', 'approve'])\n",
    "                             \n",
    "\n",
    "# TabularExplainer\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "tab_explainer = TabularExplainer(model=loan_model,\n",
    "                             initialization_examples=X_test,\n",
    "                             features=['loan_amount','income','age','marital_status'],\n",
    "                             classes=['reject', 'approve'])\n",
    "\n",
    "\n",
    "# PFIExplainer\n",
    "from interpret.ext.blackbox import PFIExplainer\n",
    "\n",
    "pfi_explainer = PFIExplainer(model = loan_model,\n",
    "                             features=['loan_amount','income','age','marital_status'],\n",
    "                             classes=['reject', 'approve'])\n",
    "```\n",
    "\n",
    "\n",
    "**<h2>Explaining global feature importance</h2>**\n",
    "\n",
    "To retrieve global importance values for the features in your model, you call the **explain_global()** method of your explainer to get a global explanation, and then use the **get_feature_importance_dict()** method to get a dictionary of the feature importance values. The following code example shows how to retrieve global feature importance:\n",
    "```\n",
    "# MimicExplainer\n",
    "global_mim_explanation = mim_explainer.explain_global(X_train)\n",
    "global_mim_feature_importance = global_mim_explanation.get_feature_importance_dict()\n",
    "\n",
    "\n",
    "# TabularExplainer\n",
    "global_tab_explanation = tab_explainer.explain_global(X_train)\n",
    "global_tab_feature_importance = global_tab_explanation.get_feature_importance_dict()\n",
    "\n",
    "\n",
    "# PFIExplainer\n",
    "global_pfi_explanation = pfi_explainer.explain_global(X_train, y_train)\n",
    "global_pfi_feature_importance = global_pfi_explanation.get_feature_importance_dict()\n",
    "```\n",
    "\n",
    "<mark>**Note:** The code is the same for MimicExplainer and TabularExplainer. The PFIExplainer requires the actual labels that correspond to the test features.</mark>\n",
    "\n",
    "\n",
    "**<h2>Explaining local feature importance</h2>**\n",
    "\n",
    "To retrieve local feature importance from a **MimicExplainer** or a **TabularExplainer**, you must call the **explain_local()** method of your explainer, specifying the subset of cases you want to explain. Then you can use the **get_ranked_local_names()** and **get_ranked_local_values()** methods to retrieve dictionaries of the feature names and importance values, ranked by importance. The following code example shows how to retrieve local feature importance:\n",
    "\n",
    "```\n",
    "# MimicExplainer\n",
    "local_mim_explanation = mim_explainer.explain_local(X_test[0:5])\n",
    "local_mim_features = local_mim_explanation.get_ranked_local_names()\n",
    "local_mim_importance = local_mim_explanation.get_ranked_local_values()\n",
    "\n",
    "\n",
    "# TabularExplainer\n",
    "local_tab_explanation = tab_explainer.explain_local(X_test[0:5])\n",
    "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
    "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
    "```\n",
    "\n",
    "<mark>**Note:** The code is the same for **MimicExplainer** and **TabularExplainer**. The **PFIExplainer** doesn't support local feature importance explanations.</mark>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oybO5zZ-q0ex"
   },
   "source": [
    "**<center><h1>Creating explanations</h1></center>**\n",
    "\n",
    "When you use an estimator or a script to train a model in an Azure Machine Learning experiment, you can create an explainer and upload the explanation it generates to the run for later analysis.\n",
    "\n",
    "**<h2>Creating an explanation in the experiment script</h2>**\n",
    "\n",
    "To create an explanation in the experiment script, you'll need to ensure that the **azureml-interpret** and **azureml-contrib-interpret** packages are installed in the run environment. Then you can use these to create an explanation from your trained model and upload it to the run outputs. The following code example shows how code to generate and upload a model explanation can be incorporated into an experiment script.\n",
    "```\n",
    "# Import Azure ML run library\n",
    "from azureml.core.run import Run\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "# other imports as required\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# code to train model goes here\n",
    "\n",
    "# Get explanation\n",
    "explainer = TabularExplainer(model, X_train, features=features, classes=labels)\n",
    "explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# Get an Explanation Client and upload the explanation\n",
    "explain_client = ExplanationClient.from_run(run)\n",
    "explain_client.upload_model_explanation(explanation, comment='Tabular Explanation')\n",
    "\n",
    "# Complete the run\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "**<h2>Viewing the explanation\n",
    "</h2>**\n",
    "\n",
    "You can view the explanation you created for your model in the **Explanations** tab for the run in Azure Machine learning studio.\n",
    "\n",
    "You can also use the **ExplanationClient** object to download the explanation in Python.\n",
    "```\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run_id(workspace=ws,\n",
    "                                       experiment_name=experiment.experiment_name, \n",
    "                                       run_id=run.id)\n",
    "explanation = client.download_model_explanation()\n",
    "feature_importances = explanation.get_feature_importance_dict()\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZutB97qFq0wN"
   },
   "source": [
    "**<center><h1>Visualizing explanations</h1></center>**\n",
    "\n",
    "Model explanations in Azure Machine Learning studio include multiple visualizations that you can use to explore feature importance.\n",
    "\n",
    "<mark>**Note:** Visualizations are only available for experiment runs that were configured to generate and upload explanations. When using automated machine learning, only the run producing the best model has explanations generated by default.</mark>\n",
    "\n",
    "**<h2>Visualizing global feature importance</h2>**\n",
    "\n",
    "The first visualization on the Explanations tab for a run shows global feature importance.\n",
    "\n",
    "<img src=\"images/09-00-vis-global.png\"/>\n",
    "\n",
    "You can use the slider to show only the top N features.\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Visualizing summary importance</h2>**\n",
    "\n",
    "Switching to the Summary Importance visualization shows the distribution of individual importance values for each feature across the test dataset.\n",
    "\n",
    "<img src=\"images/09-00-vis-summary.png\"/>\n",
    "\n",
    "You can view the features as a swarm plot (shown above), a box plot, or a violin plot.\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Visualizing local feature importance</h2>**\n",
    "\n",
    "Selecting an individual data point shows the local feature importance for the case to which the data point belongs.\n",
    "\n",
    "<img src=\"images/09-00-vis-local.png\" />\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Gs9lv7Sq0_l"
   },
   "source": [
    "**<center><h1>Interpret models</h1></center>**\n",
    "\n",
    "Now it's your chance to interpret models.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Generate feature importance for a model.\n",
    "- Generate explanations as part of a model training experiment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "\n",
    "Follow these instructions to complete the exercise.\n",
    "\n",
    "1. If you do not already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com.\n",
    "2. View the exercise repo at https://aka.ms/mslearn-dp100.\n",
    "3. If you have not already done so, complete the Create an Azure Machine Learning workspace exercise to provision an Azure Machine Learning workspace, create a compute instance, and clone the required files.\n",
    "4. Complete the Interpret models exercise.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayAa6TEBMjXT"
   },
   "source": [
    "**<center><h1>Knowledge check</h1></center>**\n",
    "\n",
    "1. You have trained a classification model, and you want to quantify the influence of each feature on a specific individual prediction. What should you examine?\n",
    "\n",
    "  - Global feature importance\n",
    "\n",
    "  - Local feature importance\n",
    "\n",
    "  - Recall and Precision\n",
    "\n",
    "2. Which explainer uses an architecture-appropriate SHAP algorithm to interpret a model?\n",
    "\n",
    "  - PFIExplainer\n",
    "\n",
    "  - MimicExplainer\n",
    "\n",
    "  - TabularExplainer\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKpz-P7vMpAW"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "In this module, you learned how to:\n",
    "\n",
    "- Interpret global and local feature importance.\n",
    "- Use an explainer to interpret a model.\n",
    "- Create model explanations in a training experiment.\n",
    "- Visualize model explanations.\n",
    "\n",
    "To learn more about interpreting models, see [Model interpretability in Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability) in the Azure Machine Learning documentation.\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course3_Module11ExplainmachinelearningmodelswithAzureMachineLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
