{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "In many production scenarios, long-running tasks that operate on large volumes of data are performed as batch operations. In machine learning, batch inferencing is used to apply a predictive model to multiple cases asynchronously - usually writing the results to a file or database.\n",
    "\n",
    "\n",
    "<img src = \"images\\07-02-batch.png\" />\n",
    "\n",
    "In Azure Machine Learning, you can implement batch inferencing solutions by creating a pipeline that includes a step to read the input data, load a registered model, predict labels, and write the results as its output.\n",
    "\n",
    "**<h2>Learning objectives</h2>**\n",
    "\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Publish batch inference pipeline for a trained model.\n",
    "- Use a batch inference pipeline to generate predictions.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-BA4eG9ECki"
   },
   "source": [
    "**<center><h1>Creating a batch inference pipeline</h1></center>**\n",
    "\n",
    "\n",
    "To create a batch inferencing pipeline, perform the following tasks:\n",
    "\n",
    "**<h2> 1. Register a model</h2>**\n",
    "\n",
    "To use a trained model in a batch inferencing pipeline, you must register it in your Azure Machine Learning workspace.\n",
    "\n",
    "To register a model from a local file, you can use the **register** method of the **Model** object as shown in the following example code:\n",
    "\n",
    "```\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=your_workspace,\n",
    "                                      model_name='classification_model',\n",
    "                                      model_path='model.pkl', # local path\n",
    "                                      description='A classification model')\n",
    "```\n",
    "\n",
    "Alternatively, if you have a reference to the **Run** used to train the model, you can use its **register_model** method as shown in the following example code:\n",
    "\n",
    "```\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')\n",
    "```\n",
    "\n",
    "**<h2> 2. Create a scoring script</h2>**\n",
    "\n",
    "Batch inferencing service requires a scoring script to load the model and use it to predict new values. It must include two functions:\n",
    "\n",
    "- **init():** Called when the pipeline is initialized.\n",
    "- **run(mini_batch):** Called for each batch of data to be processed.\n",
    "Typically, you use the init function to load the model from the model registry, and use the run function to generate predictions from each batch of data and return the results. The following example script shows this pattern:\n",
    "\n",
    "```\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for model input\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList\n",
    "```\n",
    "\n",
    "\n",
    "**<h2> 3. Create a pipeline with a ParallelRunStep </h2>**\n",
    "\n",
    "Azure Machine Learning provides a type of pipeline step specifically for performing parallel batch inferencing. Using the **ParallelRunStep** class, you can read batches of files from a **File** dataset and write the processing output to a **OutputFileDatasetConfig**. Additionally, you can set the **output_action** setting for the step to \"append_row\", which will ensure that all instances of the step being run in parallel will collate their results to a single output file named parallel_run_step.txt:\n",
    "\n",
    "```\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets['batch-data']\n",
    "\n",
    "# Set the output location\n",
    "output_dir = OutputFileDatasetConfig(name='inferences')\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='batch_scripts',\n",
    "    entry_script=\"batch_scoring_script.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=aml_cluster,\n",
    "    node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "```\n",
    "\n",
    "\n",
    "**<h2> 4. Run the pipeline and retrieve the step output </h2>**\n",
    "\n",
    "After your pipeline has been defined, you can run it and wait for it to complete. Then you can retrieve the **parallel_run_step.txt** file from the output of the step to view the results, as shown in the following code example:\n",
    "\n",
    "```\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTCc30ynGTtf"
   },
   "source": [
    "**<center><h1>Publishing a batch inference pipeline</center></h1>**\n",
    "\n",
    "You can publish a batch inferencing pipeline as a REST service, as shown in the following example code:\n",
    "\n",
    "```\n",
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    "                                                   description='Batch pipeline',\n",
    "                                                   version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "```\n",
    "\n",
    "Once published, you can use the service endpoint to initiate a batch inferencing job, as shown in the following example code:\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "```\n",
    "You can also schedule the published pipeline to have it run automatically, as shown in the following example code:\n",
    "\n",
    "```\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                                        description='batch inferencing',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Batch_Prediction',\n",
    "                                        recurrence=weekly)\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF20A7buG-mx"
   },
   "source": [
    "**<center><h1>Exercise - Create a batch inference pipeline</h1></center>**\n",
    "\n",
    "Now it's your chance to create and run a batch inferencing pipeline.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Create a batch inferencing pipeline.\n",
    "- Publish the pipeline as a REST service.\n",
    "- Run the pipeline through its REST endpoint.\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "\n",
    "Follow these instructions to complete the exercise.\n",
    "\n",
    "1. If you do not already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com.\n",
    "2. View the exercise repo at https://aka.ms/mslearn-dp100.\n",
    "3. If you have not already done so, complete the **Create an Azure Machine Learning workspace** exercise to provision an Azure Machine Learning workspace, create a compute instance, and clone the required files.\n",
    "4. Complete the Create a batch inference service exercise.\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGnlcM5RHZ6l"
   },
   "source": [
    "**<center><h1>Knowledge check</h1></center>**\n",
    "\n",
    "1. You are creating a batch inferencing pipeline that you want to use to predict new values for a large volume of data files. You want the pipeline to run the scoring script on multiple nodes and collate the results. What kind of step should you include in the pipeline?\n",
    "\n",
    "- PythonScriptStep\n",
    "\n",
    "- ParallelRunStep\n",
    "\n",
    "- AdlaStep\n",
    "\n",
    "2. You have configured the step in your batch inferencing pipeline with an output_action=\"append_row\" property. In which file should you look for the batch inferencing results?\n",
    "\n",
    "- output.txt\n",
    "\n",
    "- stdoutlogs.txt\n",
    "\n",
    "- parallel_run_step.txt\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpWi0zdeH0qL"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "\n",
    "\n",
    "In this module, you learned how to:\n",
    "\n",
    "- Publish batch inference pipeline for a trained model.\n",
    "- Use a batch inference pipeline to generate predictions.\n",
    "\n",
    "<hr></hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course3_Module7DeploybatchinferencepipelineswithAzureMachineLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
