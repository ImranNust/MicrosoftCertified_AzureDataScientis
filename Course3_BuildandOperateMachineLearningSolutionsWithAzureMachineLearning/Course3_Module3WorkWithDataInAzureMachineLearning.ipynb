{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "Data is a fundamental element in any machine learning workload, so in this module, you will learn how to create and manage datastores and datasets in an Azure Machine Learning workspace, and how to use them in model training experiments.\n",
    "\n",
    "**<h2>Learning objectives</h2>**\n",
    "\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Create and use datastores\n",
    "- Create and use datasets\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxg3lzAWelt6"
   },
   "source": [
    "**<center><h1>Introduction to datastores</h1></center>**\n",
    "\n",
    "In Azure Machine Learning, datastores are abstractions for cloud data sources. They encapsulate the information required to connect to data sources. You can access datastores directly in code by using the Azure Machine Learning SDK, and use it to upload or download data.\n",
    "\n",
    "**<h2>Types of Datastore</h2>**\n",
    "\n",
    "Azure Machine Learning supports the creation of datastores for multiple kinds of Azure data source, including:\n",
    "\n",
    "- Azure Storage (blob and file containers)\n",
    "- Azure Data Lake stores\n",
    "- Azure SQL Database\n",
    "- Azure Databricks file system (DBFS)\n",
    "\n",
    "<mark>Note: For a full list of supported datastores, see the Azure [Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#access-data-in-storage).</mark>\n",
    "\n",
    "**<h2>Built-in Datastores</h2>**\n",
    "\n",
    "Every workspace has two built-in datastores (an Azure Storage blob container, and an Azure Storage file container) that are used as system storage by Azure Machine Learning. There's also a third datastore that gets added to your workspace if you make use of the open datasets provided as samples (for example, by creating a designer pipeline based on a sample dataset)\n",
    "\n",
    "In most machine learning projects, you will likely need to work with data sources of your own - either because you need to store larger volumes of data than the built-in datastores support, or because you need to integrate your machine learning solution with data from existing applications.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQLt0l0efDu5"
   },
   "source": [
    "**<center><h1>Use datastores\n",
    "Completed</h1></center>**\n",
    "\n",
    "\n",
    "To add a datastore to your workspace, you can register it using the graphical interface in Azure Machine Learning studio, or you can use the Azure Machine Learning SDK. For example, the following code registers an Azure Storage blob container as a datastore named **blob_data**.\n",
    "```\n",
    "# Python Code\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Register a new datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                  datastore_name='blob_data', \n",
    "                                                  container_name='data_container',\n",
    "                                                  account_name='az_store_acct',\n",
    "                                                  account_key='123456abcde789â€¦')\n",
    "                                    \n",
    "```\n",
    "\n",
    "\n",
    "**<h2>Managing datastores</h2>**\n",
    "\n",
    "You can view and manage datastores in Azure Machine Learning Studio, or you can use the Azure Machine Learning SDK. For example, the following code lists the names of each datastore in the workspace.\n",
    "```\n",
    "# Python Code\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)\n",
    "```\n",
    "You can get a reference to any datastore by using the **Datastore.get()** method as shown here:\n",
    "```\n",
    "# PYthon Code\n",
    "blob_store = Datastore.get(ws, datastore_name='blob_data')\n",
    "```\n",
    "The workspace always includes a default datastore (initially, this is the built-in **workspaceblobstore** datastore), which you can retrieve by using the **get_default_datastore()** method of a **Workspace** object, like this:\n",
    "```\n",
    "# Python Code\n",
    "default_store = ws.get_default_datastore()\n",
    "```\n",
    "\n",
    "**<h2>Considerations for datastores</h2>**\n",
    "\n",
    "When planning for datastores, consider the following guidelines:\n",
    "\n",
    "- When using Azure blob storage, premium level storage may provide improved I/O performance for large datasets. However, this option will increase cost and may limit replication options for data redundancy.\n",
    "- When working with data files, although CSV format is very common, Parquet format generally results in better performance.\n",
    "- You can access any datastore by name, but you may want to consider changing the default datastore (which is initially the built-in **workspaceblobstore** datastore).\n",
    "\n",
    "To change the default datastore, use the **set_default_datastore()** method:\n",
    "```\n",
    "# Python\n",
    "ws.set_default_datastore('blob_data')\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfsG1GiMf_hn"
   },
   "source": [
    "**<center><h1>Introduction to datasets</h1></center>**\n",
    "\n",
    "Datasets are versioned packaged data objects that can be easily consumed in experiments and pipelines. Datasets are the recommended way to work with data, and are the primary mechanism for advanced Azure Machine Learning capabilities like data labeling and data drift monitoring.\n",
    "\n",
    "**<h2>Types of dataset</h2>**\n",
    "\n",
    "Datasets are typically based on files in a datastore, though they can also be based on URLs and other sources. You can create the following types of dataset:\n",
    "\n",
    "- **Tabular:** The data is read from the dataset as a table. You should use this type of dataset when your data is consistently structured and you want to work with it in common tabular data structures, such as Pandas dataframes.\n",
    "- **File:** The dataset presents a list of file paths that can be read as though from the file system. Use this type of dataset when your data is unstructured, or when you need to process the data at the file level (for example, to train a convolutional neural network from a set of image files).\n",
    "\n",
    "\n",
    "**<h2>Creating and registering datasets</h2>**\n",
    "\n",
    "You can use the visual interface in Azure Machine Learning studio or the Azure Machine Learning SDK to create datasets from individual files or multiple file paths. The paths can include wildcards (for example, /files/*.csv) making it possible to encapsulate data from a large number of files in a single dataset.\n",
    "\n",
    "After you've created a dataset, you can register it in the workspace to make it available for use in experiments and data processing pipelines later.\n",
    "\n",
    "**<h2>Creating and registering tabular datasets</h2>**\n",
    "\n",
    "To create a tabular dataset using the SDK, use the **from_delimited_files** method of the **Dataset.Tabular** class, like this:\n",
    "```\n",
    "# Python\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),\n",
    "             (blob_ds, 'data/files/archive/*.csv')]\n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)\n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')\n",
    "```\n",
    "The dataset in this example includes data from two file paths within the default datastore:\n",
    "\n",
    "- The **current_data.csv** file in the **data/files** folder.\n",
    "- All .csv files in the **data/files/archive/** folder.\n",
    "\n",
    "After creating the dataset, the code registers it in the workspace with the name **csv_table**.\n",
    "\n",
    "\n",
    "**<h2>Creating and registering file datasets</h2>**\n",
    "\n",
    "To create a file dataset using the SDK, use the **from_files** method of the **Dataset.File** class, like this:\n",
    "```\n",
    "# Python\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')\n",
    "```\n",
    "The dataset in this example includes all .jpg files in the **data/files/images** path within the default datastore:\n",
    "\n",
    "After creating the dataset, the code registers it in the workspace with the name **img_files**.\n",
    "\n",
    "**<h2>Retrieving a registered dataset</h2>**\n",
    "\n",
    "After registering a dataset, you can retrieve it by using any of the following techniques:\n",
    "\n",
    "- The **datasets** dictionary attribute of a **Workspace** object.\n",
    "- The **get_by_name** or **get_by_id** method of the **Dataset** class.\n",
    "\n",
    "Both of these techniques are shown in the following code:\n",
    "```\n",
    "# Python\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a dataset from the workspace datasets collection\n",
    "ds1 = ws.datasets['csv_table']\n",
    "\n",
    "# Get a dataset by name from the datasets class\n",
    "ds2 = Dataset.get_by_name(ws, 'img_files')\n",
    "```\n",
    "\n",
    "**<h2>Dataset versioning</h2>**\n",
    "\n",
    "Datasets can be versioned, enabling you to track historical versions of datasets that were used in experiments, and reproduce those experiments with data in the same state.\n",
    "\n",
    "You can create a new version of a dataset by registering it with the same name as a previously registered dataset and specifying the **create_new_version** property:\n",
    "```\n",
    "# Python\n",
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    "             (blob_ds, 'data/files/images/*.png')]\n",
    "file_ds = Dataset.File.from_files(path=img_paths)\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)\n",
    "```\n",
    "\n",
    "In this example, the .png files in the **images** folder have been added to the definition of the **img_paths** dataset example used in the previous topic.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Retrieving a specific dataset version</h2>**\n",
    "\n",
    "You can retrieve a specific version of a dataset by specifying the **version** parameter in the **get_by_name** method of the **Dataset** class.\n",
    "```\n",
    "# Python\n",
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIDVJ9MIhrpN"
   },
   "source": [
    "**<center><h1>Use datasets</h1></center>**\n",
    "\n",
    "Datasets are the primary way to pass data to experiments that train models.\n",
    "\n",
    "**<h2>Work with tabular datasets</h2>**\n",
    "\n",
    "You can read data directly from a tabular dataset by converting it into a Pandas or Spark dataframe:\n",
    "```\n",
    "# Python\n",
    "df = tab_ds.to_pandas_dataframe()\n",
    "# code to work with dataframe goes here, for example:\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "\n",
    "**<h3>Pass a tabular dataset to an experiment script</h3>**\n",
    "\n",
    "When you need to access a dataset in an experiment script, you must pass the dataset to the script. There are two ways you can do this.\n",
    "\n",
    "**<h4>Use a script argument for a tabular dataset</h4>**\n",
    "\n",
    "You can pass a tabular dataset as a script argument. When you take this approach, the argument received by the script is the unique ID for the dataset in your workspace. In the script, you can then get the workspace from the run context and use it to retrieve the dataset by it's ID.\n",
    "\n",
    "ScriptRunConfig:\n",
    "\n",
    "```\n",
    "# Python\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds],\n",
    "                                environment=env)\n",
    "```\n",
    "Script:\n",
    "```\n",
    "# Python\n",
    "from azureml.core import Run, Dataset\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "dataset = Dataset.get_by_id(ws, id=args.dataset_id)\n",
    "data = dataset.to_pandas_dataframe()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**<h4>Use a named input for a tabular dataset</h4>**\n",
    "\n",
    "Alternatively, you can pass a tabular dataset as a named input. In this approach, you use the **as_named_input** method of the dataset to specify a name for the dataset. Then in the script, you can retrieve the dataset by name from the run context's **input_datasets** collection without needing to retrieve it from the workspace. Note that if you use this approach, you still need to include a script argument for the dataset, even though you donâ€™t actually use it to retrieve the dataset.\n",
    "\n",
    "ScriptRunConfig:\n",
    "```\n",
    "# Python\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds.as_named_input('my_dataset')],\n",
    "                                environment=env)\n",
    "```\n",
    "Script:\n",
    "```\n",
    "# Python\n",
    "from azureml.core import Run\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_id')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "dataset = run.input_datasets['my_dataset']\n",
    "data = dataset.to_pandas_dataframe()\n",
    "```\n",
    "\n",
    "\n",
    "**<h2>Work with file datasets</h2>**\n",
    "\n",
    "When working with a file dataset, you can use the to_path() method to return a list of the file paths encapsulated by the dataset:\n",
    "```\n",
    "# Python\n",
    "for file_path in file_ds.to_path():\n",
    "    print(file_path)\n",
    "```\n",
    "**<h3>Pass a file dataset to an experiment script</h3>**\n",
    "Just as with a Tabular dataset, there are two ways you can pass a file dataset to a script. However, there are some key differences in the way that the dataset is passed.\n",
    "\n",
    "\n",
    "**<h4>Use a script argument for a file dataset</h4>**\n",
    "\n",
    "You can pass a file dataset as a script argument. Unlike with a tabular dataset, you must specify a mode for the file dataset argument, which can be **as_download** or as_mount. This provides an access point that the script can use to read the files in the dataset. In most cases, you should use **as_download**, which copies the files to a temporary location on the compute where the script is being run. However, if you are working with a large amount of data for which there may not be enough storage space on the experiment compute, use **as_mount** to stream the files directly from their source.\n",
    "\n",
    "ScriptRunConfig:\n",
    "```\n",
    "# Python\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_download()],\n",
    "                                environment=env)\n",
    "```\n",
    "Script:\n",
    "\n",
    "```\n",
    "# Python\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "imgs = glob.glob(args.ds_ref + \"/*.jpg\")\n",
    "```\n",
    "\n",
    "**<h4>Use a named input for a file dataset</h4>**\n",
    "\n",
    "You can also pass a file dataset as a named input. In this approach, you use the **as_named_input** method of the dataset to specify a name before specifying the access mode. Then in the script, you can retrieve the dataset by name from the run context's **input_datasets** collection and read the files from there. As with tabular datasets, if you use a named input, you still need to include a script argument for the dataset, even though you donâ€™t actually use it to retrieve the dataset.\n",
    "\n",
    "ScriptRunConfig:\n",
    "```\n",
    "# Python\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_named_input('my_ds').as_download()],\n",
    "                                environment=env)\n",
    "```\n",
    "\n",
    "Script:\n",
    "```\n",
    "# Python\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "dataset = run.input_datasets['my_ds']\n",
    "imgs= glob.glob(dataset + \"/*.jpg\")\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9ZweiuSjkBG"
   },
   "source": [
    "**<center><h1>Exercise - Work with data</h1></center>**\n",
    "\n",
    "\n",
    "Now it's your chance to work with data in Azure Machine Learning.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Upload data to a datastore\n",
    "- Create datasets\n",
    "- Use datasets to train a model\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "\n",
    "Follow these instructions to complete the exercise.\n",
    "\n",
    "1. If you do not already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com.\n",
    "2. View the exercise repo at https://aka.ms/mslearn-dp100.\n",
    "3. If you have not already done so, complete the **Create an Azure Machine Learning workspace** exercise to provision an Azure Machine Learning workspace, create a compute instance, and clone the required files.\n",
    "4. Complete the **Work with data** exercise."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course3_Module3WorkWithDataInAzureMachineLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
