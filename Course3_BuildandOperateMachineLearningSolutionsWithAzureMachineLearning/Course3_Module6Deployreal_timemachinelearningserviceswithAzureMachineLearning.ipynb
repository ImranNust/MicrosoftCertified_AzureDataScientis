{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course3_Module6Deployreal-timemachinelearningserviceswithAzureMachineLearning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<center><h1>Introduction</h1></center>**\n",
        "\n",
        "In machine learning, inferencing refers to the use of a trained model to predict labels for new data on which the model has not been trained. Often, the model is deployed as part of a service that enables applications to request immediate, or real-time, predictions for individual, or small numbers of data observations.\n",
        "\n",
        "<img src = \"images/07-01-real-time.jpeg\" />\n",
        "\n",
        "In Azure Machine Learning, you can create real-time inferencing solutions by deploying a model as a service, hosted in a containerized platform, such as Azure Kubernetes Services (AKS).\n",
        "\n",
        "**<h2>Learning objectives</h2>**\n",
        "\n",
        "In this module, you will learn how to:\n",
        "\n",
        "- Deploy a model as a real-time inferencing service.\n",
        "- Consume a real-time inferencing service.\n",
        "- Troubleshoot service deployment\n",
        "<hr>"
      ],
      "metadata": {
        "id": "Wc_79hyqeW1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<center><h1>Deploy a model as a real-time service</h1></center>**\n",
        "\n",
        "ou can deploy a model as a real-time web service to several kinds of compute target, including local compute, an Azure Machine Learning compute instance, an Azure Container Instance (ACI), an Azure Kubernetes Service (AKS) cluster, an Azure Function, or an Internet of Things (IoT) module. Azure Machine Learning uses containers as a deployment mechanism, packaging the model and the code to use it as an image that can be deployed to a container in your chosen compute target.\n",
        "\n",
        "<mark>Note: Deployment to a local service, a compute instance, or an ACI is a good choice for testing and development. For production, you should deploy to a target that meets the specific performance, scalability, and security needs of your application architecture.</mark>\n",
        "\n",
        "To deploy a model as a real-time inferencing service, you must perform the following tasks:\n",
        "\n",
        "**<h2>1. Register a trained model</h2>**\n",
        "\n",
        "After successfully training a model, you must register it in your Azure Machine Learning workspace. Your real-time service will then be able to load the model when required.\n",
        "\n",
        "To register a model from a local file, you can use the **register** method of the **Model** object as shown here:\n",
        "```\n",
        "# Pytyhon\n",
        "from azureml.core import Model\n",
        "\n",
        "classification_model = Model.register(workspace=ws,\n",
        "                       model_name='classification_model',\n",
        "                       model_path='model.pkl', # local path\n",
        "                       description='A classification model')\n",
        "```\n",
        "Alternatively, if you have a reference to the **Run** used to train the model, you can use its **register_model** method as shown here:\n",
        "```\n",
        "# Pytyhon\n",
        "run.register_model( model_name='classification_model',\n",
        "                    model_path='outputs/model.pkl', # run outputs path\n",
        "                    description='A classification model')\n",
        "```\n",
        "**<h2>2. Define an inference configuration</h2>**\n",
        "\n",
        "The model will be deployed as a service that consist of:\n",
        "\n",
        "- A script to load the model and return predictions for submitted data.\n",
        "- An environment in which the script will be run.\n",
        "\n",
        "You must therefore define the script and environment for the service.\n",
        "\n",
        "**<h3>Create an entry script</h3>**\n",
        "\n",
        "Create the entry script (sometimes referred to as the scoring script) for the service as a Python (.py) file. It must include two functions:\n",
        "\n",
        "- **init():** Called when the service is initialized.\n",
        "- **run(raw_data):** Called when new data is submitted to the service.\n",
        "\n",
        "Typically, you use the init function to load the model from the model registry, and use the run function to generate predictions from the input data. The following example script shows this pattern:\n",
        "```\n",
        "# Pytyhon\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Called when the service is loaded\n",
        "def init():\n",
        "    global model\n",
        "    # Get the path to the registered model file and load it\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "# Called when a request is received\n",
        "def run(raw_data):\n",
        "    # Get the input data as a numpy array\n",
        "    data = np.array(json.loads(raw_data)['data'])\n",
        "    # Get a prediction from the model\n",
        "    predictions = model.predict(data)\n",
        "    # Return the predictions as any JSON serializable format\n",
        "    return predictions.tolist()\n",
        "```\n",
        "Save the script in a folder so you can easily identify it later. For example, you might save the script above as score.py in a folder named service_files.\n",
        "\n",
        "**<h3>Create an environment</h3>**\n",
        "\n",
        "Your service requires a Python environment in which to run the entry script, which you can define by creating an **Environment** that contains the required packages:\n",
        "```\n",
        "# Pytyhon\n",
        "from azureml.core import Environment\n",
        "\n",
        "service_env = Environment(name='service-env')\n",
        "python_packages = ['scikit-learn', 'numpy'] # whatever packages your entry script uses\n",
        "for package in python_packages:\n",
        "    service_env.python.conda_dependencies.add_pip_package(package)\n",
        "```\n",
        "\n",
        "**<h3>Combine the script and environment in an InferenceConfig</h3>**\n",
        "\n",
        "After creating the entry script and environment, you can combine them in an **InferenceConfig** for the service like this:\n",
        "```\n",
        "# Pytyhon\n",
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "classifier_inference_config = InferenceConfig(source_directory = 'service_files',\n",
        "                                              entry_script=\"score.py\",\n",
        "                                              environment=service_env)\n",
        "```\n",
        "\n",
        "**<h2>3. Define a deployment configuration</h2>**\n",
        "\n",
        "Now that you have the entry script and environment, you need to configure the compute to which the service will be deployed. If you are deploying to an AKS cluster, you must create the cluster and a compute target for it before deploying:\n",
        "```\n",
        "# Python\n",
        "from azureml.core.compute import ComputeTarget, AksCompute\n",
        "\n",
        "cluster_name = 'aks-cluster'\n",
        "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
        "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "production_cluster.wait_for_completion(show_output=True)\n",
        "```\n",
        "With the compute target created, you can now define the deployment configuration, which sets the **target-specific** compute specification for the containerized deployment:\n",
        "```\n",
        "# Python\n",
        "from azureml.core.webservice import AksWebservice\n",
        "\n",
        "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
        "                                                              memory_gb = 1)\n",
        "                                    \n",
        "```\n",
        "The code to configure an ACI deployment is similar, except that you do not need to explicitly create an ACI compute target, and you must use the **deploy_configuration** class from the **azureml.core.webservice.AciWebservice **namespace. Similarly, you can use the **azureml.core.webservice.LocalWebservice** namespace to configure a local Docker-based service.\n",
        "\n",
        "<mark>Note: To deploy a model to an Azure Function, you do not need to create a deployment configuration. Instead, you need to package the model based on the type of function trigger you want to use. This functionality is in preview at the time of writing. For more details, see Deploy a machine learning model to Azure Functions in the Azure Machine Learning documentation.</mark>\n",
        "\n",
        "**<h2>4. Deploy the model</h2>**\n",
        "\n",
        "After all of the configuration is prepared, you can deploy the model. The easiest way to do this is to call the **deploy** method of the **Model** class, like this:\n",
        "```\n",
        "# Python\n",
        "from azureml.core.model import Model\n",
        "\n",
        "model = ws.models['classification_model']\n",
        "service = Model.deploy(workspace=ws,\n",
        "                       name = 'classifier-service',\n",
        "                       models = [model],\n",
        "                       inference_config = classifier_inference_config,\n",
        "                       deployment_config = classifier_deploy_config,\n",
        "                       deployment_target = production_cluster)\n",
        "service.wait_for_deployment(show_output = True)\n",
        "```\n",
        "\n",
        "For ACI or local services, you can omit the **deployment_target** parameter (or set it to **None**).\n",
        "\n",
        "<hr>\n",
        "\n"
      ],
      "metadata": {
        "id": "b-3ZBrdYCkrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<center><h1>Consume a real-time inferencing service</h1></center>**\n",
        "\n",
        "After deploying a real-time service, you can consume it from client applications to predict labels for new data cases.\n",
        "\n",
        "**<h2>Use the Azure Machine Learning SDK</h2>**\n",
        "\n",
        "For testing, you can use the Azure Machine Learning SDK to call a web service through the **run** method of a **WebService** object that references the deployed service. Typically, you send data to the **run** method in JSON format with the following structure:\n",
        "\n",
        "\n",
        "```\n",
        "# JSON\n",
        "{\n",
        "  \"data\":[\n",
        "      [0.1,2.3,4.1,2.0], // 1st case\n",
        "      [0.2,1.8,3.9,2.1],  // 2nd case,\n",
        "      ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "The response from the **run** method is a JSON collection with a prediction for each case that was submitted in the data. The following code sample calls a service and displays the response:\n",
        "\n",
        "```\n",
        "# Python\n",
        "import json\n",
        "\n",
        "# An array of new data cases\n",
        "x_new = [[0.1,2.3,4.1,2.0],\n",
        "         [0.2,1.8,3.9,2.1]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "json_data = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Call the web service, passing the input data\n",
        "response = service.run(input_data = json_data)\n",
        "\n",
        "# Get the predictions\n",
        "predictions = json.loads(response)\n",
        "\n",
        "# Print the predicted class for each case.\n",
        "for i in range(len(x_new)):\n",
        "    print (x_new[i], predictions[i])\n",
        "```\n",
        "\n",
        "**<h2>Use a REST endpoint</h2>**\n",
        "\n",
        "In production, most client applications will not include the Azure Machine Learning SDK, and will consume the service through its REST interface. You can determine the endpoint of a deployed service in Azure Machine Learning studio, or by retrieving the **scoring_uri** property of the **Webservice** object in the SDK, like this:\n",
        "```\n",
        "# Python\n",
        "endpoint = service.scoring_uri\n",
        "print(endpoint)\n",
        "```\n",
        "With the endpoint known, you can use an HTTP POST request with JSON data to call the service. The following example shows how to do this using Python:\n",
        "```\n",
        "# Python\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# An array of new data cases\n",
        "x_new = [[0.1,2.3,4.1,2.0],\n",
        "         [0.2,1.8,3.9,2.1]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "json_data = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Set the content type in the request headers\n",
        "request_headers = { 'Content-Type':'application/json' }\n",
        "\n",
        "# Call the service\n",
        "response = requests.post(url = endpoint,\n",
        "                         data = json_data,\n",
        "                         headers = request_headers)\n",
        "\n",
        "# Get the predictions from the JSON response\n",
        "predictions = json.loads(response.json())\n",
        "\n",
        "# Print the predicted class for each case.\n",
        "for i in range(len(x_new)):\n",
        "    print (x_new[i]), predictions[i] )\n",
        "```\n",
        "\n",
        "**<h2>Authentication</h2>**\n",
        "\n",
        "In production, you will likely want to restrict access to your services by applying authentication. There are two kinds of authentication you can use:\n",
        "\n",
        " - **Key:** Requests are authenticated by specifying the key associated with the service.\n",
        "- **Token:** Requests are authenticated by providing a JSON Web Token (JWT).\n",
        "By default, authentication is disabled for ACI services, and set to key-based authentication for AKS services (for which primary and secondary keys are automatically generated). You can optionally configure an AKS service to use token-based authentication (which is not supported for ACI services).\n",
        "\n",
        "Assuming you have an authenticated session established with the workspace, you can retrieve the keys for a service by using the **get_keys** method of the **WebService** object associated with the service:\n",
        "```\n",
        "# Python\n",
        " primary_key, secondary_key = service.get_keys()\n",
        "```\n",
        "For token-based authentication, your client application needs to use service-principal authentication to verify its identity through Azure Active Directory (Azure AD) and call the get_token method of the service to retrieve a time-limited token.\n",
        "\n",
        "To make an authenticated call to the service's REST endpoint, you must include the key or token in the request header like this:\n",
        "\n",
        "```\n",
        "# Python\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# An array of new data cases\n",
        "x_new = [[0.1,2.3,4.1,2.0],\n",
        "         [0.2,1.8,3.9,2.1]]\n",
        "\n",
        "# Convert the array to a serializable list in a JSON document\n",
        "json_data = json.dumps({\"data\": x_new})\n",
        "\n",
        "# Set the content type in the request headers\n",
        "request_headers = { \"Content-Type\":\"application/json\",\n",
        "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
        "\n",
        "# Call the service\n",
        "response = requests.post(url = endpoint,\n",
        "                         data = json_data,\n",
        "                         headers = request_headers)\n",
        "\n",
        "# Get the predictions from the JSON response\n",
        "predictions = json.loads(response.json())\n",
        "\n",
        "# Print the predicted class for each case.\n",
        "for i in range(len(x_new)):\n",
        "    print (x_new[i]), predictions[i] )\n",
        "```"
      ],
      "metadata": {
        "id": "Bi0dk5C5Ck8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***<center><h1>Troubleshoot service deployment</h1></center>**\n",
        "\n",
        "There are a lot of elements to a real-time service deployment, including the trained model, the runtime environment configuration, the scoring script, the container image, and the container host. Troubleshooting a failed deployment or an error when consuming a deployed service can be complex.\n",
        "\n",
        "**<h2>Check the service state</h2>**\n",
        "\n",
        "As an initial troubleshooting step, you can check the status of a service by examining its **state**:\n",
        "\n",
        "```\n",
        "#Python\n",
        "from azureml.core.webservice import AksWebservice\n",
        "\n",
        "# Get the deployed service\n",
        "service = AksWebservice(name='classifier-service', workspace=ws)\n",
        "\n",
        "# Check its state\n",
        "print(service.state)\n",
        "```\n",
        "\n",
        "<mark>Note: To view the state of a service, you must use the compute-specific service type (for example AksWebservice) and not a generic WebService object.</mark>\n",
        "\n",
        "For an operational service, the state should be Healthy.\n",
        "\n",
        "**<h2>Review service logs</h2>**\n",
        "If a service is not healthy, or you are experiencing errors when using it, you can review its logs:\n",
        "```\n",
        "#Python\n",
        "print(service.get_logs())\n",
        "```\n",
        "\n",
        "The logs include detailed information about the provisioning of the service, and the requests it has processed. They can often provide an insight into the cause of unexpected errors.\n",
        "\n",
        "**<h2>Deploy to a local container</h2>**\n",
        "\n",
        "Deployment and runtime errors can be easier to diagnose by deploying the service as a container in a local Docker instance, like this:\n",
        "```\n",
        "#Python\n",
        "from azureml.core.webservice import LocalWebservice\n",
        "\n",
        "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
        "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)\n",
        "```\n",
        "You can then test the locally deployed service using the SDK:\n",
        "```\n",
        "# Python\n",
        "print(service.run(input_data = json_data))\n",
        "```\n",
        "You can then troubleshoot runtime issues by making changes to the scoring file that is referenced in the inference configuration, and reloading the service without redeploying it (something you can only do with a local service):\n",
        "```\n",
        "#Python\n",
        "service.reload()\n",
        "print(service.run(input_data = json_data))\n",
        "```"
      ],
      "metadata": {
        "id": "GPNqPJk-Ck-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<center><h1>Exercise - Deploy a model as a real-time service</h1></center>**\n",
        "\n",
        "Now it's your chance to use Azure Machine Learning to deploy a machine learning model as a real-time service.\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "- Train and register a model.\n",
        "- Deploy the model as a real-time service.\n",
        "- Consume the deployed service.\n",
        "\n",
        "\n",
        "**<h2>Instructions</h2>**\n",
        "\n",
        "Follow these instructions to complete the exercise.\n",
        "\n",
        "1. If you do not already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com.\n",
        "2. View the exercise repo at https://aka.ms/mslearn-dp100.\n",
        "3. If you have not already done so, complete the **Create an Azure Machine Learning workspace** exercise to provision an Azure Machine Learning workspace, create a compute instance, and clone the required files.\n",
        "4. Complete the **Create a real-time inference** exercise.\n"
      ],
      "metadata": {
        "id": "CyVDJwFI3V09"
      }
    }
  ]
}