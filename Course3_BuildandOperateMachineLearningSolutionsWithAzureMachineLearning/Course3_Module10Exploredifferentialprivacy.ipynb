{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "Data science projects, including machine learning projects, involve analysis of data; and often that data includes sensitive personal details that should be kept private. In practice, most reports that are published from the data include aggregations of the data, which you may think would provide some privacy – after all, the aggregated results do not reveal the individual data values.\n",
    "\n",
    "However, consider a case where multiple analyses of the data result in reported aggregations that when combined, could be used to work out information about individuals in the source dataset. Suppose 10 participants share data about their location and salary, from which two reports are produced:\n",
    "\n",
    "- An aggregated salary report that tells us the average salaries in New York, San Francisco, and Seattle\n",
    "- A worker location report that tells us that 10% of the study participants (in other words, a single person) is based in Seattle.\n",
    "\n",
    "<img src = \"images/09-reveal-analysis.png\" />\n",
    "\n",
    "From these two reports, we can easily determine the specific salary of the Seattle-based participant. Anyone reviewing both studies who happens to know a person from Seattle that participated, now knows that person's salary.\n",
    "\n",
    "In this module, you'll explore differential privacy, a technique that can help protect an individual's data against this kind of exposure.\n",
    "\n",
    "**<h2>Learning objectives</h2>**\n",
    "\n",
    "In this module, you will learn how to:\n",
    "\n",
    "- Articulate the problem of data privacy\n",
    "- Describe how differential privacy works\n",
    "- Configure parameters for differential privacy\n",
    "- Perform differentially private data analysis\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNd_35m1ovKT"
   },
   "source": [
    "**<center><h1>Understand differential privacy</h1></center>**\n",
    "\n",
    "Differential privacy seeks to protect individual data values by adding statistical \"noise\" to the analysis process. The math involved in adding the noise is complex, but the principle is fairly intuitive – the noise ensures that data aggregations stay statistically consistent with the actual data values allowing for some random variation, but make it impossible to work out the individual values from the aggregated data. In addition, the noise is different for each analysis, so the results are non-deterministic – in other words, two analyses that perform the same aggregation may produce slightly different results.\n",
    "\n",
    "<img src=\"images/09-differential-privacy.png\" />\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfeIzWRJovj0"
   },
   "source": [
    "**<center><h1>Configure data privacy parameters</h1></center>**\n",
    "\n",
    "\n",
    "One way that an individual can protect their personal data is simply to not participate in a study – this is known as their \"opt-out\" option. However, there are a few considerations for this as a solution:\n",
    "\n",
    "- Even if you decide to opt out a study may still produce results that affect you. For example, you may choose to opt-out of a study that compares the heart disease diagnoses across a group of people on the basis that doing so may reveal a heart disease diagnosis that causes your health insurance premiums to rise. If the study finds a correlation between people who drink coffee and higher risk of heart disease, and your insurance company knows that you are a coffee drinker, your rate may rise even though you didn’t personally participate in the study.\n",
    "- The benefits of participation in the study may outweigh any negative impact. For example, if you're paid $100 to participate in a study that results in your health insurance rate rising by $10 per year, it will be more than 10 years before you make a net loss. This may be a worthwhile tradeoff to you (particularly if your rate may rise as a result of the study even if you don’t participate!)\n",
    "- The only way for the opt-out option to work for every individual, is for every individual not to take part – which makes the whole study pointless!\n",
    "\n",
    "The amount of variation caused by adding noise is configurable through a parameter called epsilon. This value governs the amount of additional risk that your personal data can be identified through rejecting the opt-out option and participating in a study. The key thing is that it applies this privacy principle for everyone participating in the study. A low epsilon value provides the most privacy, at the expense of less accuracy when aggregating the data. A higher epsilon value results in aggregations that are more true to the actual data distribution, but in which the individual contribution of a single individual to the aggregated value is less obscured by noise.\n",
    "\n",
    "<img src=\"images/epsilon.png\" />\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIANdIRtov21"
   },
   "source": [
    "**<center><h1>Use differential privacy</h1></center>**\n",
    "\n",
    "Now it's your chance to explore differential privacy for yourself by using the SmartNoise package.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Use SmartNoise to generate differentially private analyses.\n",
    "- Use SmartNoise to submit differentially private queries.\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "\n",
    "Follow these instructions to complete the exercise.\n",
    "\n",
    "1. If you do not already have an Azure subscription, sign up for a free trial at https://azure.microsoft.com.\n",
    "2. View the exercise repo at https://aka.ms/mslearn-dp100.\n",
    "3. If you have not already done so, complete the Create an Azure Machine Learning workspace exercise to provision an Azure Machine Learning workspace, create a compute instance, and clone the required files.\n",
    "4. Complete the Explore differential privacy exercise.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayAa6TEBMjXT"
   },
   "source": [
    "**<center><h1>Knowledge check</h1></center>**\n",
    "\n",
    "How does differential privacy work?\n",
    "\n",
    "- All numeric values in the dataset are encrypted and cannot be used in analysis.\n",
    "\n",
    "- Noise is added to the data during analysis so that aggregations are statistically consistent with the data distribution but non-deterministic.\n",
    "\n",
    "- All numeric column values in the dataset are converted to the mean value for the column. Analyses of the data use the mean values instead of the actual values.\n",
    "\n",
    "2. In a differential privacy solution, what is the effect of setting an epsilon parameter?\n",
    "\n",
    "- A lower epsilon reduces the impact of an individual's data on aggregated results, increasing privacy and reducing accuracy\n",
    "\n",
    "- A lower epsilon reduces the amount of noise added to the data, increasing accuracy and reducing privacy\n",
    "\n",
    "- Setting epsilon to 1 enables differential privacy. Setting it to 0 disables differential privacy.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKpz-P7vMpAW"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "In this module, you learned how to:\n",
    "\n",
    "- Articulate the problem of data privacy\n",
    "- Describe how differential privacy works\n",
    "- Configure parameters for differential privacy\n",
    "- Perform differentially private data analysis\n",
    "\n",
    "To learn more about interpreting models, see [Differential Privacy](https://docs.microsoft.com/en-us/azure/machine-learning/concept-differential-privacy) in the Azure Machine Learning documentation.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course3_Module10Exploredifferentialprivacy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
