{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "Azure Databricks is a Microsoft analytics service, part of the Microsoft Azure cloud platform. It offers an integration between Microsoft Azure and the Apache Spark's Databricks implementation. Azure Databricks natively integrates with Azure security and data services. In this module, you will learn how to work with the key features of Azure Databricks.\n",
    "\n",
    "\n",
    "**<h2>Learning Objectives</h2>**\n",
    "\n",
    "After completing this module, you’ll be able to:\n",
    "\n",
    "- Describe the main concepts in Azure Databricks.\n",
    "- Work with workspaces and clusters.\n",
    "- Work with notebooks.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHBmmv7zvKWk"
   },
   "source": [
    "**<center><h1>Understand Azure Databricks</h1></center>**\n",
    "\n",
    "\n",
    "Azure Databricks runs on top of a proprietary data processing engine called Databricks Runtime, an optimized version of Apache Spark. It allows up to 50x performance for Apache Spark workloads.\n",
    "\n",
    "Apache Spark is the core technology. Spark is an open-source analytics engine for large-scale data processing. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "\n",
    "In a nutshell: Azure Databricks offers a fast, easy, and collaborative Spark based analytics service. It is used to accelerate big data analytics, artificial intelligence, performant data lakes, interactive data science, machine learning, and collaboration.\n",
    "\n",
    "**<h2>The main concepts in Azure Databricks</h2>**\n",
    "\n",
    "<img src = \"images/01-01-01-databricks-workspace.jpg\"/>\n",
    "\n",
    "The landing page shows the fundamental concepts to be used in Databricks:\n",
    "\n",
    "1. The **cluster:** a set of computational resources on which we run the code.\n",
    "2. The **workspace:** groups all the Databricks elements, clusters, notebooks, data.\n",
    "3. The **notebook:** a document that contains runnable code, descriptive text, and visualizations.\n",
    " \n",
    " \n",
    "<mark>**Note:** More information: for more information about Azure Databricks, see the [documentation](https://docs.microsoft.com/en-us/azure/databricks/scenarios/what-is-azure-databricks).</mark>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7fPDDp_vKjc"
   },
   "source": [
    "**<center><h1>Provision Azure Databricks workspaces and clusters</h1></center>**\n",
    "\n",
    "Two of the key concepts you need to be familiar with when working with Azure Databricks are **workspaces** and **clusters**.\n",
    "\n",
    "\n",
    "**<h2>Workspaces</h2>**\n",
    "\n",
    "A workspace is an environment for accessing all of your Databricks elements:\n",
    "\n",
    "- It groups objects (like notebooks, libraries, experiments) into folders,\n",
    "- Provides access to your data,\n",
    "- Provides access to the computations resources used (clusters, jobs).\n",
    "\n",
    "<img src=\"images/01-01-05-workspace.png\"/>\n",
    "\n",
    "Each user has a home folder for their notebooks and libraries. The objects stored in the Workspace root folder are: folders, notebooks, libraries, and experiments.\n",
    "\n",
    "To perform an action on a Workspace object, we can right-click the object and choose one of the available actions.\n",
    "\n",
    "**<h2>Clusters</h2>**\n",
    "\n",
    "A cluster is a set of computational resources on which you run your code (as notebooks or jobs). We can run ETL pipelines, or machine learning, data science, analytics workloads on the cluster.\n",
    "\n",
    "We can create:\n",
    "\n",
    "- An **all-purpose cluster.** Multiple users can share such clusters to do collaborative interactive analysis.\n",
    "- A **job cluster** to run a specific job. The cluster will be terminated when the job completes (A job is a way of running a notebook or JAR either immediately or on a scheduled basis).\n",
    "\n",
    "Before we can use a cluster, we have to choose one of the available **runtimes**.\n",
    "\n",
    "Databricks runtimes are the set of core components that run on Azure Databricks clusters. Azure Databricks offers several types of runtimes:\n",
    "\n",
    "- **Databricks Runtime:** includes Apache Spark, components and updates that optimize the usability, performance, and security for big data analytics.\n",
    "- **Databricks Runtime for Machine Learning:** a variant that adds multiple machine learning libraries such as TensorFlow, Keras, and PyTorch.\n",
    "- **Databricks Light:** for jobs that don’t need the advanced performance, reliability, or autoscaling of the Databricks Runtime.\n",
    "\n",
    "To create and configure a new cluster, we have to select the **Create Cluster** button and choose our options.\n",
    "\n",
    "<img src=\"images/01-01-02-new-cluster.png\"/>\n",
    "\n",
    "We will see your new cluster appearing in the clusters list.\n",
    "\n",
    "<img src=\"images/01-01-03-clusters.png\"/>\n",
    "\n",
    "To launch the cluster, we have to select the **Start** button and then confirm to launch it. It is recommended to wait until the cluster is started.\n",
    "\n",
    "A cluster can be customized in many ways. In case you want to make third-party code available to your notebooks, you can install a library. Your cluster can be provisioned to use Python/Java/Scala/R libraries via PyPI or Maven.\n",
    "\n",
    "Once the cluster is running, we can select **Edit** to change its properties. In case we want to provision your cluster with additional libraries, we can select the **Libraries** and then choose **Install New**.\n",
    "\n",
    "<img src=\"images/01-01-04-provision-cluster.png\"/>\n",
    "\n",
    "We can pick a library and it will be available later to be used in your notebooks.\n",
    "\n",
    "**<h2>Working with data in a workspace</h2>**\n",
    "\n",
    "An Azure Databricks database is a collection of tables. An Azure Databricks table is a collection of structured data.\n",
    "\n",
    "We can cache, filter, and perform any operations supported by Apache Spark DataFrames on Azure Databricks tables. We can query tables with Spark APIs and Spark SQL.\n",
    "\n",
    "To access our data:\n",
    "\n",
    "- We can import our files to DBFS using the UI.\n",
    "- We can mount and use supported data sources via DBFS.\n",
    "\n",
    "We can then use Spark or local APIs to access the data.\n",
    "\n",
    "We will be able to use a DBFS file path in our notebook to access our data, independent of its data source.\n",
    "\n",
    "It is possible to import existing data or code in the workspace.\n",
    "\n",
    "If we use small data files on the local machine that we want to analyze with Azure Databricks, we can import them to DBFS using the UI. There are two ways to upload data to DBFS with the UI:\n",
    "\n",
    "- Upload files to the FileStore in the Upload Data UI.\n",
    "- Upload data to a table with the Create table UI, which is also accessible via the Import & Explore Data box on the landing page.\n",
    "\n",
    "We may also read data on cluster nodes using Spark APIs. We can read data imported to DBFS into Apache Spark DataFrames. For example, if you import a CSV file, you can read the data using this code\n",
    "```\n",
    "df = spark.read.csv('/FileStore/tables/nyc_taxi.csv', header=\"true\", inferSchema=\"true\")\n",
    "```\n",
    "We can also read data imported to DBFS in programs running on the Spark driver node using local file APIs. For example:\n",
    "```\n",
    "df = spark.read.csv('/dbfs/FileStore/tables/nyc_taxi.csv', header=\"true\", inferSchema=\"true\")\n",
    "```\n",
    "\n",
    "**<h2>Importing data</h2>**\n",
    "\n",
    "To add data, we can go to the landing page and select **Import & Explore Data**.\n",
    "\n",
    "To get the data in a table, there are multiple options available:\n",
    "\n",
    "- Upload a local file and import the data.\n",
    "- Use data already existing under DBFS.\n",
    "- Mount external data sources, like Azure Storage, Azure Data Lake and more.\n",
    "\n",
    "To create a table based on a local file, we can select **Upload File** to upload data from your local machine.\n",
    "\n",
    "<img src=\"images/01-01-06-upload.png\"/>\n",
    "\n",
    "Once the data is uploaded, it will be available as a table or as a mountpoint under the DBFS filesystem (/FileStore).\n",
    "\n",
    "Databricks can create a table automatically if we select **Create Table with UI**.\n",
    "\n",
    "<img src=\"images/01-01-07-table-ui.png\"/>\n",
    "\n",
    "Alternately, we can have full control over the structure of the new table by choosing Create Table in Notebook. Azure Databricks will generate Spark code that loads your data (and we can customize it via the Spark API).\n",
    "\n",
    "\n",
    "<img src=\"images/01-01-08-table-spark.png\"/>\n",
    "\n",
    "**<h2>Using DBFS mounted data</h2>**\n",
    "\n",
    "Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage and offers the following benefits:\n",
    "\n",
    "- Allows to you mount storage objects so that you can seamlessly access data without requiring credentials.\n",
    "- Allows you to interact with object storage using directory and file semantics instead of storage URLs.\n",
    "- Persists files to object storage, so you won’t lose data after you terminate a cluster.\n",
    "\n",
    "The default storage location in DBFS is known as the DBFS root.\n",
    "\n",
    "We can use the DBFS to access:\n",
    "\n",
    "- Local files (previously imported). For example, the tables you imported above are available under `/FileStore`\n",
    "- Remote files, objects kept in separate storages as if they were on the local file system\n",
    "\n",
    "For example, to mount a remote Azure storage account as a DBFS folder, we can use the `dbutils` module:\n",
    "```\n",
    "data_storage_account_name = '<data_storage_account_name>'\n",
    "data_storage_account_key = '<data_storage_account_key>'\n",
    "\n",
    "data_mount_point = '/mnt/data'\n",
    "\n",
    "data_file_path = '/bronze/wwi-factsale.csv'\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://dev@{data_storage_account_name}.blob.core.windows.net\",\n",
    "  mount_point = data_mount_point,\n",
    "  extra_configs = {f\"fs.azure.account.key.{data_storage_account_name}.blob.core.windows.net\": data_storage_account_key})\n",
    "\n",
    "display(dbutils.fs.ls(\"/mnt/data\"))\n",
    "#this path is available as dbfs:/mnt/data for spark APIs, e.g. spark.read\n",
    "#this path is available as file:/dbfs/mnt/data for regular APIs, e.g. os.listdir\n",
    "```\n",
    "\n",
    "Notebooks support a shorthand - `%fs` magic command - for accessing the dbutils filesystem module. Most `dbutils.fs` commands are available using `%fs` magic commands:\n",
    "```\n",
    "# bash\n",
    "# List the DBFS root\n",
    "%fs ls\n",
    "\n",
    "# Overwrite the file \"/mnt/my-file\" with the string \"Hello world!\"\n",
    "%fs put -f \"/mnt/my-file\" \"Hello world!\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPqBhKivuvUk"
   },
   "source": [
    "**<center><h1>Work with notebooks in Azure Databricks</h1></center>**\n",
    "\n",
    "\n",
    "A notebook is a web-based interface to a document that contains:\n",
    "\n",
    "- Runnable code\n",
    "- Descriptive text\n",
    "- Visualizations\n",
    "\n",
    "A notebook is a collection of runnable cells (commands). When you use a notebook, you are primarily developing and running cells.\n",
    "\n",
    "Runnable cells operate on files and tables. Cells can be run in sequence, referring to the output of previously run cells.\n",
    "\n",
    "To create a notebook, we can select **Workspace**, browse into the desired folder, right-click, and choose **Create**, then select **Notebook**.\n",
    "\n",
    "<img src=\"images/01-01-09-new-notebook.png\"/>\n",
    "\n",
    "A name should be given to the new notebook, and a default language to be used inside the code cells. Choose a cluster to run the code in the cells.\n",
    "\n",
    "For runnable cells, the following programming languages are supported: Python, Scala, R, and SQL. You may choose the default language for the cells in a notebook. You may also override that language later.\n",
    "\n",
    "<img src=\"images/01-01-10-new-notebook.png\"/>\n",
    "\n",
    "The notebook editor opens with a first empty cell\n",
    "\n",
    "<img src=\"images/01-01-11-edit-notebook.png\"/>\n",
    "\n",
    "By hovering over the Plus button below the current cell or by choosing the top-right menu options, we can change the contents of the notebook. We may add new cells, cut, copy, export the cell contents, or run a specific cell.\n",
    "\n",
    "We can override the default language by specifying the language magic command `%<language>` at the beginning of a cell.\n",
    "\n",
    "The supported magic commands are:\n",
    "\n",
    "- %python\n",
    "- %r\n",
    "- %scala\n",
    "- %sql\n",
    "\n",
    "Notebooks also support a few auxiliary magic commands:\n",
    "\n",
    "- `%sh`: Allows you to run shell code in your notebook\n",
    "- `%fs`: Allows you to use dbutils filesystem commands\n",
    "- `%md`: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjVieX0LvKIV"
   },
   "source": [
    "**<center><h1>Exercise - Get started with Azure Databricks</h1></center>**\n",
    "\n",
    "\n",
    "Now it's your chance to get started with Azure Databricks for yourself by configuring a cluster, creating a workspace and a notebook.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Create an Azure Databricks Cluster.\n",
    "- Provision an Azure Databricks Workspace.\n",
    "- Work with Notebooks.\n",
    "-  Use DBFS.\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "\n",
    "Follow these instructions to complete the exercise:\n",
    "\n",
    "1. Open the exercise instructions at https://aka.ms/mslearn-dp090.\n",
    "2. Complete the **Getting Started with Azure Databricks** exercise.\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emC3beVXvJ6N"
   },
   "source": [
    "**<center><h1>Knowledge check</h1></center>**\n",
    "\n",
    "\n",
    "Choose the best response for each of the questions below. Then select Check your answers.\n",
    "\n",
    "1. Alice creates a notebook on Azure Databricks to train her datasets, before using them with Spark ML. Which of the following languages are supported for doing that in a notebook?\n",
    "\n",
    " - Java\n",
    "\n",
    " - Python\n",
    "\n",
    " - C#\n",
    "\n",
    "2. You want to train a neural network with TensorFlow. You don't want to install the library manually to avoid extra overhead. What should you do?\n",
    "\n",
    " -  Create a cluster with the Databricks Runtime for Machine Learning.\n",
    "\n",
    " -  Create a single node cluster.\n",
    "\n",
    " - Create a Python notebook.\n",
    "\n",
    "3. Which description of DBFS is correct?\n",
    "\n",
    " - You can upload a file to the DBFS using the UI.\n",
    "\n",
    " - You can only access data in Azure Databricks if it's stored on DBFS.\n",
    "\n",
    " - Data uploaded to the DBFS is only stored as long as your cluster is running.\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQqz8pHUzriv"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "\n",
    "In this module, you have learned how to get started with Azure Databricks.\n",
    "\n",
    "Now that you've completed this module, you can:\n",
    "\n",
    "- Describe the main concepts in Azure Databricks.\n",
    "- Work with workspaces and clusters.\n",
    "- Work with notebooks.\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course4_Module1GetstartedwithAzureDatabricks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
