{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "In Azure Databricks, data scientists use DataFrames to structure their data. A DataFrame is equivalent to a relational table in Spark SQL. In this module, you will learn what [DataFrames](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame?azure-portal=true) are and how to use them.\n",
    "\n",
    "\n",
    "**<h2>Learning Objectives</h2>**\n",
    "\n",
    "After completing this module, youâ€™ll be able to:\n",
    "\n",
    "- Describe DataFrames.\n",
    "- Query DataFrames.\n",
    "- Visualize data with Spark.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPqBhKivuvUk"
   },
   "source": [
    "**<center><h1>Understand dataframes</h1></center>**\n",
    "\n",
    "Spark uses 3 different APIs: RDDs, DataFrames, and DataSets. The architectural foundation is the resilient distributed dataset (RDD). The DataFrame API was released as an abstraction on top of the RDD, followed later by the Dataset API. We'll only use DataFrames in our notebook examples.\n",
    "\n",
    "DataFrames are the distributed collections of data, organized into rows and columns. Each column in a DataFrame has a name and an associated type.\n",
    "\n",
    "Spark DataFrames can be created from various sources, such as CSV files, JSON, Parquet files, Hive tables, log tables, and external databases.\n",
    "\n",
    "\n",
    "**<h2>Using Spark to load table data</h2>**\n",
    "\n",
    "Assuming we have this data available in a table:\n",
    "\n",
    "<img src=\"images/01-02-01-select-sql.png\" />\n",
    "\n",
    "We can use Spark to load the table data by using the sql method:\n",
    "```\n",
    "df = spark.sql(\"SELECT * FROM nyc_taxi_csv\")\n",
    "```\n",
    "Spark supports many different data formats, such as CSV, JSON, XML, Parquet, Avro, ORC and more.\n",
    "\n",
    "\n",
    "**<h2>DataFrame size</h2>**\n",
    "\n",
    "To get the number of rows available in a DataFrame, we can use the ```count()``` method.\n",
    "```\n",
    "df.count\n",
    "```\n",
    "\n",
    "**<h2>DataFrame structure</h2>**\n",
    "\n",
    "To get the schema metadata for a given DataFrame, we can use the ```printSchema()``` method.\n",
    "\n",
    "Each column in a given DataFrame has a name, a type, and a nullable flag.\n",
    "```\n",
    "df.printSchema\n",
    "```\n",
    "\n",
    "<img src=\"images/01-02-04-spark-dataframe-printschema.png\" />\n",
    "\n",
    "**<h2>DataFrame contents</h2>**\n",
    "\n",
    "Spark has a built-in function that allows to print the rows inside a DataFrame: ```show()```\n",
    "```\n",
    "df.show\n",
    "df.show(100, truncate=False) #show more lines, do not truncate\n",
    "```\n",
    "By default it will only show the first 20 lines in your DataFrame and it will truncate long columns. Additional parameters are available to override these settings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exJkxNHb05Kv"
   },
   "source": [
    "**<center><h1>Query dataframes</h1></center>**\n",
    "\n",
    "DataFrames allow the processing of huge amounts of data. Spark uses an optimization engine to generate logical queries. Data is distributed over your cluster and you get huge performance for massive amounts of data.\n",
    "\n",
    "Spark SQL is a component that introduced the DataFrames, which provides support for structured and semi-structured data.\n",
    "\n",
    "Spark has multiple interfaces (APIs) for dealing with DataFrames:\n",
    "\n",
    "- We have seen the .sql() method, which allows to run arbitrary SQL queries on table data.\n",
    "- Another option is to use the Spark domain-specific language for structured data manipulation, available in Scala, Java, Python, and R.\n",
    "\n",
    "\n",
    "**<h2>DataFrame API</h2>**\n",
    "\n",
    "The Apache Spark DataFrame API provides a rich set of functions (select columns, filter, join, aggregate, and so on) that allow you to solve common data analysis problems efficiently.\n",
    "\n",
    "A complex operation where tables are joined, filtered, and restructured is easy to write, easy to understand, type safe, feels natural for people with prior sql experience, and comes with the added speed of parallel processing given by the Spark engine.\n",
    "\n",
    "To load or save data use read and ```write```:\n",
    "```\n",
    "df = spark.read.format('json').load('sample/trips.json')\n",
    "df.write.format('parquet').bucketBy(100, 'year', 'month').mode(\"overwrite\").saveAsTable('table1'))\n",
    "```\n",
    "To get the available data in a DataFrame use ```select```:\n",
    "```\n",
    "df.select('*')\n",
    "df.select('tripDistance', 'totalAmount')\n",
    "```\n",
    "To extract the first rows, use ```take```:\n",
    "```\n",
    "df.take(15)\n",
    "```\n",
    "To order the data, use the sort ```method```:\n",
    "```\n",
    "df.sort(df.tripDistance.desc())\n",
    "```\n",
    "To combine the rows in multiple DataFrames use ```union```:\n",
    "```\n",
    "df1.union(df2)\n",
    "```\n",
    "This operation is equivalent to ```UNION ALL``` in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by ```distinct()```.\n",
    "\n",
    "The dataframes must have the same structure/schema.\n",
    "\n",
    "To add or update columns use ```withColumn``` or ```withColumnRenamed```:\n",
    "```\n",
    "df.withColumn('isHoliday', False)\n",
    "df.withColumnRenamed('isDayOff', 'isHoliday')\n",
    "```\n",
    "To use aliases for the whole DataFrame or specific columns:\n",
    "```\n",
    "df.alias(\"myTrips\")\n",
    "df.select(df.passengerCount.alias(\"numberOfPassengers\"))\n",
    "```\n",
    "\n",
    "To create a temporary view:\n",
    "```\n",
    "df.createOrReplaceTempView(\"tripsView\")\n",
    "```\n",
    "To aggregate on the entire DataFrame without groups use ```agg```:\n",
    "```\n",
    "df.agg({\"age\": \"max\"})\n",
    "```\n",
    "\n",
    "To do more complex queries, use ```filter```, ```groupBy``` and ```join```:\n",
    "\n",
    "```\n",
    "people \\\n",
    "  .filter(people.age > 30) \\\n",
    "  .join(department, people.deptId == department.id) \\\n",
    "  .groupBy(department.name, \"gender\")\n",
    "  .agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
    "\n",
    "```\n",
    "These join types are supported: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti.\n",
    "\n",
    "Note that ```filter``` is an alias for ```where```.\n",
    "\n",
    "To use columns aggregations using windows:\n",
    "\n",
    "```\n",
    "w = Window.partitionBy(\"name\").orderBy(\"age\").rowsBetween(-1, 1)\n",
    "df.select(rank().over(w), min('age').over(window))\n",
    "```\n",
    "To use a list of conditions for a column and return an expression use ```when```:\n",
    "\n",
    "```\n",
    "df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
    "```\n",
    "\n",
    "To check the presence of data use ```isNull``` or ```isNotNull```:\n",
    "\n",
    "```\n",
    "df.filter(df.passengerCount.isNotNull())\n",
    "df.filter(df.totalAmount.isNull())\n",
    "```\n",
    "To clean the data use ```dropna```, ```fillna``` or ```dropDuplicates```:\n",
    "\n",
    "```\n",
    "df1.fillna(1) #replace nulls with specified value\n",
    "df2.dropna #drop rows containing null values\n",
    "df3.dropDuplicates #drop duplicate rows\n",
    "```\n",
    "To get statistics about the DataFrame use ```summary``` or ```describe```:\n",
    "\n",
    "```\n",
    "df.summary().show()\n",
    "df.summary(\"passengerCount\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
    "df.describe(['age']).show()\n",
    "```\n",
    "\n",
    "Available statistics are:\n",
    "\n",
    "- Count\n",
    "- Mean\n",
    "- Stddev\n",
    "- Min\n",
    "- Max\n",
    "- Arbitrary approximate percentiles specified as a percentage (for example, 75%).\n",
    "\n",
    "To find correlations between specific columns use corr. This operation currently only supports the Pearson Correlation Coefficient:\n",
    "\n",
    "```\n",
    "df.corr('tripDistance', 'totalAmount')\n",
    "```\n",
    "<mark>**Note: More information: for more information about the Spark API, see the [DataFrame API](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame?azure-portal=true) and the [Column API](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.Column?azure-portal=true) in the Spark documentation.</mark>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJny5sl05Yf"
   },
   "source": [
    "**<center><h1>Visualize data</h1></center>**\n",
    "\n",
    "Spark has a built-in ```show``` function, which allows to print the rows in a DataFrame.\n",
    "\n",
    "Azure Databricks adds its own display capabilities and adds various other types of visualizations out-of-the-box using the ```display``` and ```displayHTML``` functions.\n",
    "\n",
    "The same data we've seen previously as a table can be displayed as a bar chart, pie, histogram, or other graphs. Even maps or images can be displayed:\n",
    "\n",
    "<img src=\"images/01-02-07-spark-dataframe-view.png\" />\n",
    "\n",
    "\n",
    "**<h2>Plot options</h2>**\n",
    "\n",
    "The following display options are available:\n",
    "\n",
    "- We can choose the DataFrame columns to be used as axes (keys, values).\n",
    "- We can choose to group our series of data.\n",
    "- We can choose the aggregations to be used with our grouped data (avg, sum, count, min, max).\n",
    "\n",
    "<img src=\"images/01-02-08-spark-dataframe-view-plot-options.png\" />\n",
    "\n",
    "<mark>**Note:** More information: for more information about the available visualizations, see Visualizations in the Azure Databricks documentation.</mark>\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B75umyJ05k3"
   },
   "source": [
    "**<center><h1>Exercise - Work with data in Azure Databricks</h1></center>**\n",
    "\n",
    "Now it's your chance to experience Azure Databricks for yourself by loading data, manipulating it and visualizing the results.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Load data into a DataFrame.\n",
    "- Query a dataframe.\n",
    "- Transform data using DataFrames.\n",
    "- Visualize data.\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "Follow these instructions to complete the exercise:\n",
    "\n",
    "1. Open the exercise instructions at https://aka.ms/mslearn-dp090.\n",
    "2. Complete the Working with **Data in Azure Databricks** exercise.\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHPkQune05uW"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "In this module, you have learned how to work with data on Azure Databricks.\n",
    "\n",
    "Now that you've completed this module, you can:\n",
    "\n",
    "- Describe DataFrames.\n",
    "- Query DataFrames.\n",
    "- Visualize data with Spark.\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Course4_Module2WorkwithdatainAzureDatabricks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
