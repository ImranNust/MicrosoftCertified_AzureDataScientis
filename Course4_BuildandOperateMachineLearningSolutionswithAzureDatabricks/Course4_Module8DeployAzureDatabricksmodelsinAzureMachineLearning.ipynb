{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc_79hyqeW1G"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "After training your model on Azure Databricks Compute, you may want to deploy your model so that it can be consumed by your business or end user. You can easily deploy your model by using Azure Machine Learning. In this module, you will learn how to deploy models using Azure Databricks and Azure Machine Learning.\n",
    "\n",
    "\n",
    "**<h2>Learning Objectives</h2>**\n",
    "\n",
    "After completing this module, you'll be able to:\n",
    "\n",
    "- Describe considerations for model deployment.\n",
    "- Plan for deployment endpoints.\n",
    "- Deploy a model as an inferencing webservice.\n",
    "- Troubleshoot model deployment.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPqBhKivuvUk"
   },
   "source": [
    "**<center><h1>Describe considerations for model deployment</h1></center>**\n",
    "\n",
    "In machine learning, Model Deployment can be considered as a process by which you integrate your trained machine learning models into a production environment such that your business or end-user applications can use the model predictions to make decisions or gain insights into your data. The most common way you deploy a model using Azure Machine Learning from Azure Databricks, is to deploy the model as a real-time inferencing service. Here the term inferencing refers to the use of a trained model to make predictions on new input data on which the model has not been trained.\n",
    "\n",
    "**<h2>What is Real-Time Inferencing?</h2>**\n",
    "\n",
    "The model is deployed as part of a service that enables applications to request immediate, or real-time, predictions for individual, or small numbers of data observations.\n",
    "\n",
    "<img src=\"images/04-02-01-real-time.jpg\" />\n",
    "\n",
    "In Azure Machine learning, you can create real-time inferencing solutions by deploying a model as a real-time service, hosted in a containerized platform such as Azure Kubernetes Services (AKS).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AOYGD4704w4"
   },
   "source": [
    "**<center><h1>Plan for Azure Machine Learning deployment endpoints</h1></center>**\n",
    "\n",
    "\n",
    "After you have trained your machine learning model and evaluated it to the point where you are ready to use it outside your own development or test environment, you need to deploy it somewhere. Azure Machine Learning service simplifies this process. You can use the service components and tools to register your model and deploy it to one of the available **compute targets** so it can be made available as a web service in the Azure cloud, or on an IoT Edge device.\n",
    "\n",
    "\n",
    "**<h2>Available compute targets</h2>**\n",
    "\n",
    "You can use the following compute targets to host your web service deployment:\n",
    "\n",
    "\n",
    "<img src=\"images/image5.png\" />\n",
    "\n",
    "**<h2>Deploy a model to Azure Machine Learning</h2>**\n",
    "\n",
    "As we discussed in the previous unit, you can deploy a model to several kinds of compute target: including local compute, an Azure Container Instance (ACI), an Azure Kubernetes Service (AKS) cluster, or an Internet of Things (IoT) module. Azure Machine Learning uses containers as a deployment mechanism, packaging the model and the code to use it as an image that can be deployed to a container in your chosen compute target.\n",
    "\n",
    "To deploy a model as an inferencing webservice, you must perform the following tasks:\n",
    "\n",
    "1. Register a trained model.\n",
    "2. Define an Inference Configuration.\n",
    "3. Define a Deployment Configuration.\n",
    "4. Deploy the Model.\n",
    "\n",
    "\n",
    "**<h2>1. Register a trained model</h2>**\n",
    "\n",
    "After successfully training a model, you must register it in your Azure Machine Learning workspace. Your real-time service will then be able to load the model when required.\n",
    "\n",
    "To register a model from a local file, you can use the **register** method of the **Model** object as shown here:\n",
    "\n",
    "```\n",
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(workspace=ws, \n",
    "                       model_name='nyc-taxi-fare',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='Model to predict taxi fares in NYC.')\n",
    "```\n",
    "\n",
    "**<h2>2. Define an Inference Configuration</h2>**\n",
    "\n",
    "The model will be deployed as a service that consists of:\n",
    "\n",
    "- A script to load the model and return predictions for submitted data.\n",
    "- An environment in which the script will be run.\n",
    "\n",
    "You must therefore define the script and environment for the service.\n",
    "\n",
    "**<h3>Creating an Entry Script</h3>**\n",
    "\n",
    "Create the entry script (sometimes referred to as the scoring script) for the service as a Python (.py) file. It must include two functions:\n",
    "\n",
    "- **init():** Called when the service is initialized.\n",
    "- **run(raw_data):** Called when new data is submitted to the service.\n",
    "Typically, you use the **init** function to load the model from the model registry, and use the **run** function to generate predictions from the input data. The following example script shows this pattern:\n",
    "\n",
    "```\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = Model.get_model_path('nyc-taxi-fare')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()\n",
    "```\n",
    "\n",
    "**<h3>Creating an Environment</h3>**\n",
    "\n",
    "Azure Machine Learning environments are an encapsulation of the environment where your machine learning training happens. They define Python packages, environment variables, Docker settings and other attributes in declarative fashion. The below code snippet shows an example of how you can create an environment for your deployment:\n",
    "\n",
    "```\n",
    "from azureml.core import Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "\n",
    "my_env_name=\"nyc-taxi-env\"\n",
    "myenv = Environment.get(workspace=ws, name='AzureML-Minimal').clone(my_env_name)\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"numpy==1.18.1\")\n",
    "conda_dep.add_pip_package(\"pandas==1.1.5\")\n",
    "conda_dep.add_pip_package(\"joblib==0.14.1\")\n",
    "conda_dep.add_pip_package(\"scikit-learn==0.24.1\")\n",
    "conda_dep.add_pip_package(\"sklearn-pandas==2.1.0\")\n",
    "myenv.python.conda_dependencies=conda_dep\n",
    "```\n",
    "\n",
    "**<h3>Combining the Script and Environment in an InferenceConfig</h3>**\n",
    "\n",
    "After creating the entry script and environment, you can combine them in an **InferenceConfig** for the service like this:\n",
    "\n",
    "```\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "inference_config = InferenceConfig(entry_script='score.py', \n",
    "                                   source_directory='.', \n",
    "                                   environment=myenv)\n",
    "```\n",
    "\n",
    "**<h2>3. Define a Deployment Configuration</h2>**\n",
    "\n",
    "Now that you have the entry script and environment, you need to configure the compute to which the service will be deployed. If you are deploying to an AKS cluster, you must create the cluster and a compute target for it before deploying:\n",
    "\n",
    "```\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    "With the compute target created, you can now define the deployment configuration, which sets the target-specific compute specification for the containerized deployment:\n",
    "\n",
    "```\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                   memory_gb = 1)\n",
    "```\n",
    "\n",
    "The code to configure an ACI deployment is similar, except that you do not need to explicitly create an ACI compute target, and you must use the **deploy_configuration** class from the **azureml.core.webservice.AciWebservice** namespace. Similarly, you can use the **azureml.core.webservice.LocalWebservice** namespace to configure a local Docker-based service.\n",
    "\n",
    "**<h2>4. Deploy the Model</h2>**\n",
    "\n",
    "After all of the configuration is prepared, you can deploy the model. The easiest way to do this is to call the deploy method of the **Model** class, like this:\n",
    "\n",
    "```\n",
    "from azureml.core.model import Model\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'nyc-taxi-service',\n",
    "                       models = [model],\n",
    "                       inference_config = inference_config,\n",
    "                       deployment_config = deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "```\n",
    "\n",
    "For ACI or local services, you can omit the **deployment_target** parameter (or set it to **None**).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exJkxNHb05Kv"
   },
   "source": [
    "**<center><h1>Troubleshoot model deployment</h1></center>**\n",
    "\n",
    "There are a lot of elements to a service deployment, including the trained model, the runtime environment configuration, the scoring script, the container image, and the container host. Troubleshooting a failed deployment, or an error when consuming a deployed service can be complex.\n",
    "\n",
    "\n",
    "**<h2>Check the service state</h2>**\n",
    "\n",
    "As an initial troubleshooting step, you can check the status of a service by examining its **state:**\n",
    "\n",
    "```\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "# Get the deployed service\n",
    "service = AksWebservice(name='classifier-service', workspace=ws)\n",
    "\n",
    "# Check its state\n",
    "print(service.state)\n",
    "\n",
    "```\n",
    "\n",
    "<mark>**Note:** To view the state of a service, you must use the compute-specific service type (for example AksWebservice) and not a generic WebService object.</mark>\n",
    "\n",
    "For an operational service, the state should be Healthy.\n",
    "\n",
    "**<h2>Review service logs</h2>**\n",
    "\n",
    "If a service is not healthy, or you are experiencing errors when using it, you can review its logs:\n",
    "\n",
    "```\n",
    "print(service.get_logs())\n",
    "```\n",
    "\n",
    "The logs include detailed information about the provisioning of the service, and the requests it has processed; and can often provide an insight into the cause of unexpected errors.\n",
    "\n",
    "**<h2>Deploy to a local container</h2>**\n",
    "\n",
    "Deployment and runtime errors can be easier to diagnose by deploying the service as a container in a local Docker instance, like this:\n",
    "\n",
    "```\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
    "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)\n",
    "```\n",
    "You can then test the locally deployed service using the SDK:\n",
    "```\n",
    "print(service.run(input_data = json_data))\n",
    "```\n",
    "You can then troubleshoot runtime issues by making changes to the scoring file that is referenced in the inference configuration, and reloading the service without redeploying it (something you can only do with a local service):\n",
    "```\n",
    "service.reload()\n",
    "print(service.run(input_data = json_data))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJny5sl05Yf"
   },
   "source": [
    "**<center><h1>Exercise - Deploy an Azure Databricks model in Azure Machine Learning</h1></center>**\n",
    "\n",
    "Now, you will learn to train models in Azure Databricks and then deploy models in Azure Machine Learning.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Register a databricks-trained model in AML.\n",
    "- Deploy a service that uses the model.\n",
    "- Consume the deployed service.\n",
    "\n",
    "**<h2>Instructions</h2>**\n",
    "Follow these instructions to complete the exercise:\n",
    "\n",
    "1. Open the exercise instructions at https://aka.ms/mslearn-dp090.\n",
    "2. Complete the **Deploying Models in Azure Machine Learning** exercises.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course4_Module8DeployAzureDatabricksmodelsinAzureMachineLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
