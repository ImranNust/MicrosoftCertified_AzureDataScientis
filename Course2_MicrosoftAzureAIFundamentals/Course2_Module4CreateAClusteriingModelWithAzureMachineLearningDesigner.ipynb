{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7iCXlN2mjCu"
   },
   "source": [
    "**<center><h1>Introduction</h1></center>**\n",
    "\n",
    "Clustering is a form of machine learning that is used to group similar items into clusters based on their features. For example, a researcher might take measurements of penguins, and group them based on similarities in their proportions.\n",
    "\n",
    "<img src = \"images/penguins.png\"  />\n",
    "\n",
    "\n",
    "Clustering is an example of unsupervised machine learning, in which you train a model to separate items into clusters based purely on their characteristics, or features. There is no previously known cluster value (or label) from which to train the model.\n",
    "\n",
    "You can use Microsoft Azure Machine Learning designer to create clustering models by using a drag and drop visual interface, without needing to write any code.\n",
    "\n",
    "In this module, you'll learn how to:\n",
    "\n",
    "- Use Azure Machine Learning designer to train a clustering model.\n",
    "- Use a clustering model for inferencing.\n",
    "- Deploy a clustering model as a service.\n",
    "\n",
    "\n",
    "To complete this module, you'll need a Microsoft Azure subscription. If you don't already have one, you can sign up for a free trial at https://azure.microsoft.com.\n",
    "\n",
    "\n",
    "\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HMyklGkm4So"
   },
   "source": [
    "**<center><h1>Create an Azure Machine Learning workspace</h1></center>**\n",
    "\n",
    "Azure Machine Learning is a cloud-based platform for building and operating machine learning solutions in Azure. It includes a wide range of features and capabilities that help data scientists prepare data, train models, publish predictive services, and monitor their usage. One of these features is a visual interface called designer, that you can use to train, test, and deploy machine learning models without writing any code.\n",
    "\n",
    "**<h2>Create an Azure Machine Learning workspace</h2>**\n",
    "\n",
    "To use Azure Machine Learning, you create a workspace in your Azure subscription. You can then use this workspace to manage data, compute resources, code, models, and other artifacts related to your machine learning workloads.\n",
    "\n",
    "<mark>Note: This module is one of many that make use of an Azure Machine Learning workspace, including the other modules in the [Create no-code predictive models with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/) learning path. If you are using your own Azure subscription, you may consider creating the workspace once and reusing it in other modules. Your Azure subscription will be charged a small amount for data storage as long as the Azure Machine Learning workspace exists in your subscription, so we recommend you delete the Azure Machine Learning workspace when it is no longer required.</mark>\n",
    "\n",
    "If you do not already have one, follow these steps to create a workspace:\n",
    "\n",
    "1. [Sign into the Azure portal](https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize?redirect_uri=https%3A%2F%2Fportal.azure.com%2Fsignin%2Findex%2F&response_type=code%20id_token&scope=https%3A%2F%2Fmanagement.core.windows.net%2F%2Fuser_impersonation%20openid%20email%20profile&state=OpenIdConnect.AuthenticationProperties%3Dz72cT56dMDrMEzqmMXYwoKHwBE4ZVsku4nlPA6bVjj1vgK4Oq5qCoRKDFgRcmtnFolsmn6UV8SNtBPQvKbkrWLEG304KFtgu-Z0jKaiwGkfc1PYxCP2qhpuEEXT1NcNrpsSQMn4-4mjHmBPLT8tRBLuMoCDLN-g2AQOxEvgaJc8MPMwktp3SyMPGYiKu362EKXfFFzapaCtBuQsJOSp5DB3a1ulLu4Fn8mcnEMspnCD-z93f1IbSeMXqcPdnXov7_hs6Blp4Vhwmcqi-ZZywmJty4Qi2hUirVWCQR_VtX7oVCbF6iUZ4SqvxG33s8G4ll0ps6zesFWjb4i48k6wFv7hA9hv4llbUveq7KfTtnoz9ZbnqlIHKEH7t_k19CX4Qro0o8B2cWtDao0qjAKlsfA&response_mode=form_post&nonce=637836117167542965.N2NmODIwYTctNWZjMi00Nzk5LWE0ZjctZDFmNTI4ZGM2YzllZGZhYWMzNTUtYWU2OC00MzM4LTk4YzctM2MwMThjNDU3Mzg3&client_id=c44b4083-3bb0-49c1-b47d-974e53cbdf3c&site_id=501430&client-request-id=c9029c42-b8ae-4c34-894e-f5087e777224&x-client-SKU=ID_NET472&x-client-ver=6.12.2.0) using your Microsoft credentials.\n",
    "2. Select ＋ **Create a resource**, search for **Machine Learning**, and create a new Machine Learning resource the following settings:\n",
    "- **Subscription:** Your Azure subscription\n",
    "- **Resource group:** Create or select a resource group\n",
    "- **Workspace name:** Enter a unique name for your workspace\n",
    "- **Region:** Select the geographical region closest to you\n",
    "- **Storage account:** Note the default new storage account that will be created for your workspace\n",
    "- **Key vault:** Note the default new key vault that will be created for your workspace\n",
    "- **Application insights:** Note the default new application insights resource that will be created for your workspace\n",
    "- **Container registry:** None (one will be created automatically the first time you deploy a model to a container)\n",
    "\n",
    "3. Wait for your workspace to be created (it can take a few minutes). Then go to it in the portal.\n",
    "4. On the **Overview** page for your workspace, launch Azure Machine Learning Studio (or open a new browser tab and navigate to https://ml.azure.com), and sign into Azure Machine Learning studio using your Microsoft account.\n",
    "5. In Azure Machine Learning studio, toggle the ☰ icon at the top left to view the various pages in the interface. You can use these pages to manage the resources in your workspace.\n",
    "\n",
    "You can manage your workspace using the Azure portal, but for data scientists and Machine Learning operations engineers, Azure Machine Learning studio provides a more focused user interface for managing workspace resources.\n",
    "\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU5Thav6nKCg"
   },
   "source": [
    "**<center><h1>Create compute resources</h1></center>**\n",
    "\n",
    "To train and deploy models using Azure Machine Learning designer, you need compute targets to run the training process. You will also use these compute targets to test the trained model after its deployment.\n",
    "\n",
    "**<h2>Create compute targets</h2>**\n",
    "\n",
    "Compute targets are cloud-based resources on which you can run model training and data exploration processes.\n",
    "\n",
    "In [Azure Machine Learning studio](https://ml.azure.com/), view the **Compute** page (under **Manage**). You manage the compute targets for your data science activities in the studio. There are four kinds of compute resource that you can create:\n",
    "\n",
    "- **Compute Instances:** Development workstations that data scientists can use to work with data and models.\n",
    "- **Compute Clusters:** Scalable clusters of virtual machines for on-demand processing of experiment code.\n",
    "- **Inference Clusters:** Deployment targets for predictive services that use your trained models.\n",
    "- **Attached Compute:** Links to existing Azure compute resources, such as Virtual Machines or Azure Databricks clusters.\n",
    "\n",
    "<mark>Note: Compute instances and clusters are based on standard Azure virtual machine images. For this module, the Standard_DS11_v2 image is recommended to achieve the optimal balance of cost and performance. If your subscription has a quota that does not include this image, choose an alternative image; but bear in mind that a larger image may incur higher cost and a smaller image may not be sufficient to complete the tasks. Alternatively, ask your Azure administrator to extend your quota.</mark>\n",
    "\n",
    "1. On the **Compute Instances** tab, add a new compute instance with the following settings:\n",
    "\n",
    "  - **Compute name:** enter a unique name\n",
    "  - **Virtual Machine type:** CPU\n",
    "  - **Virtual Machine size:**\n",
    "    - Choose **Select from all options**\n",
    "    - Search for and select **Standard_DS11_v2**\n",
    "2. While the compute instance is being created, switch to the **Compute Clusters** tab, and add a new compute cluster with the following settings:\n",
    "\n",
    "- **Location:** Select the same as your workspace. If that location is not listed, choose the one closest to you\n",
    "- **Virtual Machine tier:** Dedicated\n",
    "- **Virtual Machine type:** CPU\n",
    "- **Virtual Machine size:**\n",
    "  - Choose **Select from all options**\n",
    "  - Search for and select **Standard_DS11_v2**\n",
    "- **Compute name:** enter a unique name\n",
    "- **Minimum number of nodes:** 0\n",
    "- **Maximum number of nodes:** 2\n",
    "- **Idle seconds before scale down:** 120\n",
    "- **Enable SSH access:** Unselected\n",
    "\n",
    "<mark>Tip: After you finish the entire module, be sure to follow the Clean Up instructions at the end of the module to stop your compute resources. Stop your compute resources to ensure your subscription won't be charged.</mark>\n",
    "\n",
    "The compute targets take some time to be created. You can move onto the next unit while you wait.\n",
    "\n",
    "<hr></hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWSQzCRxnWyz"
   },
   "source": [
    "**<center><h1>Explore data</h1></center>**\n",
    "\n",
    "To train a clustering model, you need a dataset that includes multiple observations of the items you want to cluster, including numeric features that can be used to determine similarities between individual cases that will help separate them into clusters.\n",
    "\n",
    "**<h2>Create a dataset</h2>**\n",
    "\n",
    "In Azure Machine Learning, data for model training and other operations is usually encapsulated in an object called a dataset.\n",
    "\n",
    "1. In [Azure Machine Learning studio](https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize?client_id=d7304df8-741f-47d3-9bc2-df0e24e2071f&scope=https%3A%2F%2Fmanagement.core.windows.net%2F%2F.default%20openid%20profile%20offline_access&redirect_uri=https%3A%2F%2Fml.azure.com&client-request-id=bcae5ad4-7135-473d-97ec-675793fda9f9&response_mode=fragment&response_type=code&x-client-SKU=msal.js.browser&x-client-VER=2.13.1&x-client-OS=&x-client-CPU=&client_info=1&code_challenge=ipKkhzqwGl1ngZTkHit9lInwWndStZ6Zg_39oE7jc8Y&code_challenge_method=S256&nonce=19bda47c-c0c7-4ba4-aca0-42972300e69b&state=eyJpZCI6IjRhY2NiYTdhLTM4YWUtNDZiYy05M2VjLTA0ZmQ0OWY4OGZjMSIsIm1ldGEiOnsiaW50ZXJhY3Rpb25UeXBlIjoicmVkaXJlY3QifX0%3D), view the **Datasets** page. Datasets represent specific data files or tables that you plan to work with in Azure ML.\n",
    "\n",
    "2. Create a dataset **from web files**, using the following settings:\n",
    "\n",
    "- **Basic Info:**\n",
    "  - **Web URL:** https://aka.ms/penguin-data\n",
    "  - **Name:** penguin-data\n",
    "  - **Dataset type:** Tabular\n",
    "  - **Description:** Penguin data\n",
    "- **Settings and preview:**\n",
    "  - **File format:** Delimited\n",
    "  - **Delimiter:** Comma\n",
    "  - **Encoding:** UTF-8\n",
    "  - **Column headers:** Only first file has headers\n",
    "  - **Skip rows:** None\n",
    "- **Schema:**\n",
    "  - Include all columns other than **Path**\n",
    "  - Review the automatically detected types\n",
    "- **Confirm details:**\n",
    "  - Do not profile the dataset after creation\n",
    "\n",
    "3. After the dataset has been created, open it and view the **Explore** page to see a sample of the data. This data represents measurements of the culmen (bill) length and depth, flipper length, and body mass for multiple observations of penguins. There are three species of penguin represented in the dataset: Adelie, Gentoo, and Chinstrap.\n",
    "\n",
    "**<h2>Create a pipeline</h2>**\n",
    "\n",
    "\n",
    "To get started with Azure Machine Learning designer, first you must create a pipeline and add the dataset you want to work with.\n",
    "\n",
    "1. In [Azure Machine Learning studio](https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize?client_id=d7304df8-741f-47d3-9bc2-df0e24e2071f&scope=https%3A%2F%2Fmanagement.core.windows.net%2F%2F.default%20openid%20profile%20offline_access&redirect_uri=https%3A%2F%2Fml.azure.com&client-request-id=bcae5ad4-7135-473d-97ec-675793fda9f9&response_mode=fragment&response_type=code&x-client-SKU=msal.js.browser&x-client-VER=2.13.1&x-client-OS=&x-client-CPU=&client_info=1&code_challenge=ipKkhzqwGl1ngZTkHit9lInwWndStZ6Zg_39oE7jc8Y&code_challenge_method=S256&nonce=19bda47c-c0c7-4ba4-aca0-42972300e69b&state=eyJpZCI6IjRhY2NiYTdhLTM4YWUtNDZiYy05M2VjLTA0ZmQ0OWY4OGZjMSIsIm1ldGEiOnsiaW50ZXJhY3Rpb25UeXBlIjoicmVkaXJlY3QifX0%3D) for your workspace, view the **Designer** page and select + to create a new pipeline.\n",
    "2. At the top left-hand side of the screen, click on the default pipeline name **(Pipeline-Created-on-date)** and change it to **Train Penguin Clustering**.\n",
    "3. You need to specify a compute target on which to run the pipeline. In the **Settings** pane, click on **Select compute target** to select the compute cluster you created previously (if the **Settings** pane is not visible, select the **⚙** icon next to the pipeline name at the top).\n",
    "4. Next to the pipeline name on the left, select the button >> to expand the panel. Drag the **penguin-data** dataset you created in the previous exercise onto the canvas.\n",
    "5. Right-click (Ctrl+click on a Mac) the **penguin-data** dataset on the canvas, and on the **Outputs** menu, select **Dataset output** by clicking on the Preview data graph icon.\n",
    "6. Review the schema of the data, noting that you can see the distributions of the various columns as histograms. Then select the CulmenLength column. The dataset should look similar to this:\n",
    "\n",
    "<img src=\"images/penguin-visualization.png\"/>\n",
    "\n",
    "\n",
    "\n",
    "7. Note the following characteristics of the dataset:\n",
    "\n",
    "  - The dataset includes the following columns:\n",
    " - **CulmenLength:** Length of the penguin's bill in millimeters.\n",
    "  - **CulmenDepth:** Depth of the penguin's bill in millimeters.\n",
    "  - **FlipperLength:** Length of the penguin's flipper in millimeters.\n",
    "  - **BodyMass:** Weight of the penguin in grams.\n",
    "  - **Species:** Species indicator (0:\"Adelie\", 1:\"Gentoo\", 2:\"Chinstrap\")\n",
    "There are two missing values in the **CulmenLength** column (the **CulmenDepth, FlipperLength**, and **BodyMass** columns also have two missing values).\n",
    "The measurement values are in different scales (from tens of millimeters to thousands of grams).\n",
    "8. Close the dataset visualization so you can see the dataset on the pipeline canvas.\n",
    "\n",
    "\n",
    "**<h2>Add Transformations</h2>**\n",
    "\n",
    "To cluster the penguin observations, we're going to use only the measurements; so we'll discard the species column. We also need to remove rows where values are missing, and normalize the numeric measurement values so they're on a similar scale.\n",
    "\n",
    "\n",
    "\n",
    "1. In the pane on the left, expand the **Data Transformation** section, which contains a wide range of modules you can use to transform data before model training.\n",
    "2. To cluster the penguin observations, we're going to use only the measurements - we'll ignore the species column. So, drag a **Select Columns in Dataset** module to the canvas, below the **penguin-data** module and connect the output at the bottom of the **penguin-data** module to the input at the top of the **Select Columns in Dataset** module, like this:\n",
    "\n",
    "<img src=\"images/dataset-select-columns.png\"  />\n",
    "\n",
    "3. Select the **Select Columns in Dataset** module, and in its **Settings** pane on the right, select **Edit column**. Then in the **Select columns** window, select **By name** and use the + links to select the column names **CulmenLength, CulmenDepth, FlipperLength,** and **BodyMass**; like this:\n",
    "\n",
    "<img src=\"images/select-columns.png\"  />\n",
    "\n",
    "\n",
    "4. Save the **Select Columns in a Dataset** module settings to return to the designer canvas.\n",
    "5. Drag a **Clean Missing Data** module to the canvas, below the **Select columns in a dataset** module and connect them like this:\n",
    "\n",
    "<img src=\"images/clean-missing-data.png\"  />\n",
    "\n",
    "6. Select the **Clean Missing Data** module, and in the settings pane on the right, click **Edit column**. Then in the **Select columns** window, select **With rules** and include **All columns**; like this:\n",
    "\n",
    "<img src=\"images/normalize-columns.png\" />\n",
    "\n",
    "7. With the **Clean Missing Data** module still selected, in the settings pane, set the following configuration settings:\n",
    "\n",
    "  - **Minimum missing value ratio:** 0.0\n",
    "  - **Maximum missing value ratio:** 1.0\n",
    "  - **Cleaning mode:** Remove entire row\n",
    "8. Drag a **Normalize Data** module to the canvas, below the **Clean Missing Data** module. Then connect the left-most output from the **Clean Missing Data** module to the input of the **Normalize Data** module.\n",
    "\n",
    "<img src=\"images/dataset-normalize.png\"  />\n",
    "\n",
    "9. Select the **Normalize Data** module, and in its **Settings** pane on the right, set the **Transformation method** to **MinMax** and select E**dit column**. Then in the **Select columns** window, select **With rules** and include **All columns**; like this:\n",
    "\n",
    "<img src=\"images/normalize-columns.png\"  />\n",
    "\n",
    "10. Save the **Normalize Data** module settings to return to the designer canvas.\n",
    "\n",
    "\n",
    "\n",
    "**<h2>Run the pipeline</h2>**\n",
    "\n",
    "\n",
    "To apply your data transformations, you need to run the pipeline as an experiment.\n",
    "\n",
    "1. Ensure your pipeline looks similar to this:\n",
    "\n",
    "<img src=\"images/data-preparation.png\"  />\n",
    "\n",
    "2. Select **Submit**, and run the pipeline as a new experiment named **mslearn-penguin-training** on your compute cluster.\n",
    "3. Wait for the run to finish - this may take a few minutes.\n",
    "\n",
    "<img src=\"images/normalize-complete.png\"  />\n",
    "\n",
    "\n",
    "\n",
    "**<h2>View the transformed data</h2>**\n",
    "\n",
    "The dataset is now prepared for model training.\n",
    "\n",
    "1. Select the completed **Normalize Data** module, and in its **Settings** pane on the right, on the **Outputs + logs** tab, select the Visualize icon for the **Transformed dataset**.\n",
    "2. View the data, noting that the numeric columns you selected have been normalized to a common scale.\n",
    "3. Close the normalized data result visualization.\n",
    "\n",
    "Now that you have selected and prepared the features you want to use from the dataset, you're ready to use them to train a clustering model.\n",
    "\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1zWVTK8sWfe"
   },
   "source": [
    "**<center><h1>Create and run a training pipeline</h1></center>**\n",
    "\n",
    "After you've used data transformations to prepare the data, you can use it to train a machine learning model.\n",
    "\n",
    "**<h2>Add training modules</h2>**\n",
    "\n",
    "To train a clustering model, you need to apply a clustering algorithm to the data, using only the features that you have selected for clustering. You'll train the model with a subset of the data, and use the rest to test the trained model.\n",
    "\n",
    "\n",
    "In this exercise, you're going to work through steps to extend the **Train Penguin Clustering** pipeline as shown here:\n",
    "\n",
    "<img src=\"images/k-means.png\"  />\n",
    "\n",
    "Follow the steps below, using the image above for reference as you add and configure the required modules.\n",
    "\n",
    "1. Open the **Train Penguin Clustering** pipeline you created in the previous unit if it's not already open.\n",
    "2. In the pane on the left, in the **Data Transformations** section, drag a **Split Data** module onto the canvas under the **Normalize Data** module. Then connect the Transformed Dataset (left) output of the **Normalize Data** module to the input of the **Split Data** module.\n",
    "3. Select the **Split Data** module, and configure its settings as follows:\n",
    "  - **Splitting mode** Split Rows\n",
    "  - **Fraction of rows in the first output dataset:** 0.7\n",
    "  - **Random seed:** 123\n",
    "  - **Stratified split:** False\n",
    "4. Expand the **Model Training** section in the pane on the left, and drag a **Train Clustering Model** module to the canvas, under the **Split Data** module. Then connect the Result dataset1 (left) output of the **Split Data** module to the Dataset (right) input of the **Train Clustering Model** module.\n",
    "5. The clustering model should assign clusters to the data items by using all of the features you selected from the original dataset. Select the **Train Clustering Model** module and in its settings pane, on the **Parameters** tab, select **Edit Columns** and use the **With rules** option to include all columns; like this:\n",
    "\n",
    "<img src=\"images/cluster-features.png\"  />\n",
    "\n",
    "6. The model we're training will use the features to group the data into clusters, so we need to train the model using a clustering algorithm. Expand the **Machine Learning Algorithms** section, and under **Clustering**, drag a **K-Means Clustering** module to the canvas, to the left of the penguin-data dataset and above the **Train Clustering Model** module. Then connect its output to the **Untrained model** (left) input of the **Train Clustering Model** module.\n",
    "\n",
    "7. The K-Means algorithm groups items into the number of clusters you specify - a value referred to as K. Select the **K-Means Clustering** module and in its settings pane, on the **Parameters** tab, set the **Number of centroids** parameter to 3.\n",
    "\n",
    "\n",
    "8. After using 70% of the data to train the clustering model, you can use the remaining 30% to test it by using the model to assign the data to clusters. Expand the **Model Scoring & Evaluation** section and drag an **Assign Data to Clusters** module to the canvas, below the **Train Clustering Model** module. Then connect the **Trained model** (left) output of the **Train Clustering Model** module to the **Trained model** (left) input of the **Assign Data to Clusters** module; and connect the **Results dataset2** (right) output of the **Split Data** module to the **Dataset** (right) input of the **Assign Data to Clusters module**.\n",
    "\n",
    "\n",
    "**<h2>Run the training pipeline</h2>**\n",
    "\n",
    "Now you're ready to run the training pipeline and train the model.\n",
    "\n",
    "1. Ensure your pipeline looks like this:\n",
    "\n",
    "<img src=\"images/k-means.png\"  />\n",
    "\n",
    "\n",
    "2. Select **Submit**, and run the pipeline using the existing experiment named mslearn-diabetes-training.\n",
    "3. Wait for the experiment run to finish. This may take 5 minutes or more.\n",
    "4. When the experiment run has finished, select the **Assign Data to Clusters ** module and in the settings pane, on the **Outputs + Logs** tab, under **Data outputs** in the **Results dataset ** section, use the **Preview data** icon to view the results.\n",
    "4. Scroll to the right, and note  the **Assignments** column (which contains the cluster (0, 1, or 2) to which each row is assigned. There are also new columns indicating the distance from the point representing this row to the centers of each of the clusters - the cluster to which the point is closest is the one to which it is assigned.\n",
    "6. Close the **Assign Data to Clusters** visualization.\n",
    "\n",
    "The model is predicting clusters for the penguin observations, but how reliable are its predictions? To assess that, you need to evaluate the model.\n",
    "\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIaVV03Iu7LA"
   },
   "source": [
    "**<center><h1>Evaluate a classification model\n",
    "Completed</h1></center>**\n",
    "\n",
    "\n",
    "\n",
    "Evaluating a clustering model is made difficult by the fact that there are no previously known true values for the cluster assignments. A successful clustering model is one that achieves a good level of separation between the items in each cluster, so we need metrics to help us measure that separation.\n",
    "\n",
    "\n",
    "**<h2>Add an Evaluate Model module</h2>**\n",
    "\n",
    "1. Open the **Train Penguin Clustering** pipeline you created in the previous unit if it's not already open.\n",
    "\n",
    "2. In the pane on the left, in the **Model Scoring & Evaluation** section, drag an **Evaluate Model** module to the canvas, under the **Assign Data to Clusters** module, and connect the output of the **Assign Data to Clusters** module to the **Scored dataset** (left) input of the **Evaluate Model** module.\n",
    "3. Ensure your pipeline looks like this:\n",
    "\n",
    "<img src=\"images/evaluate-cluster.png\"  />\n",
    "\n",
    "4. Select **Submit**, and run the pipeline using the existing experiment named **mslearn-penguin-training**.\n",
    "5. Wait for the experiment run to finish.\n",
    "6. When the experiment run has finished, select the **Evaluate Model** module and in the settings pane, on the **Outputs + Logs** tab, under **Data outputs** in the **Evaluation results** section, use the **Preview Data** icon to view the performance metrics. They include a row of metrics for each cluster, and a summary row for a combined evaluation. The metrics in each row are:\n",
    "\n",
    "  - **Average Distance to Other Center:** This indicates how close, on average, each point in the cluster is to the centroids of all other clusters.\n",
    "  - **Average Distance to Cluster Center:** This indicates how close, on average, each point in the cluster is to the centroid of the cluster.\n",
    "  - **Number of Points:** The number of points assigned to the cluster.\n",
    "  - **Maximal Distance to Cluster Center:** The maximum of the distances between each point and the centroid of that point’s cluster. If this number is high, the cluster may be widely dispersed. This statistic in combination with the Average Distance to Cluster Center helps you determine the cluster’s spread.\n",
    "\n",
    "\n",
    "\n",
    "7.  Close the **Evaluate Model result visualization** window.\n",
    "\n",
    "Now that you have a working clustering model, you can use it to assign new data to clusters in an inference pipeline.\n",
    "\n",
    "\n",
    "\n",
    "<hr></hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8p73ekcu6wt"
   },
   "source": [
    "**<center><h1>Create an inference pipeline</h1></center>**\n",
    "\n",
    "After creating and running a pipeline to train the clustering model, you can create an inference pipeline that uses the model to assign new data observations to clusters. This will form the basis for a predictive service that you can publish for applications to use.\n",
    "\n",
    "**<h2>Create an inference pipeline</h2>**\n",
    "\n",
    "1. In Azure Machine Learning Studio, click the **Designer** page to view all of the pipelines you have created. Then open the **Train Penguin Clustering** pipeline you created previously.\n",
    "\n",
    "2. Navigate to the the **Create inference pipeline** drop-down list, located on the top right hand corner of the screen. If you do not see it, you may need to expand your screen or click on the ... three dots which represent **More Actions** on the top right hand corner. Then click **Real-time inference pipeline**. After a few seconds, a new version of your pipeline named **Train Penguin Clustering-real time inference** will be opened.\n",
    "\n",
    "If the pipeline does not include **Web Service Input** and **Web Service Output** modules, go back to the **Designer** page and then re-open the **Train Penguin Clustering-real time inference**pipeline.\n",
    "\n",
    "3. Rename the new pipeline to **Predict Penguin Clusters**, and then review the new pipeline. It contains a web service input for new data to be submitted, and a web service output to return results. Some of the transformations and training steps have been encapsulated in this pipeline so that the statistics from your training data will be used to normalize any new data values, and the trained model will be used to score the new data.\n",
    "\n",
    "You are going to make the following changes to the inference pipeline:\n",
    "\n",
    "<img src=\"images/inference-changes.png\"  />\n",
    "\n",
    "- Replace the **penguin-data** dataset with an **Enter Data Manually** module that does not include the label column (**Species**).\n",
    "\n",
    "- Remove the **Select Columns in Dataset** module, which is now redundant.\n",
    "- Connect the **Web Service Input** and **Enter Data Manually** modules (which represent inputs for data to be clustered) to the first **Apply Transformation** module.\n",
    "\n",
    "- Remove the **Evaluate Model** module.\n",
    "\n",
    "\n",
    "Follow the remaining steps below, using the image and information above for reference as you modify the pipeline.\n",
    "\n",
    "\n",
    "4. The inference pipeline assumes that new data will match the schema of the original training data, so the **penguin-data ** dataset from the training pipeline is included. However, this input data includes a column for the penguin species, which the model does not use. Delete both the **penguin-data** dataset and the **Select Columns in Dataset** modules, and replace them with an **Enter Data Manually** module from the **Data Input and Output** section. Then modify the settings of the **Enter Data Manually** module to use the following CSV input, which contains feature values for three new penguin observations (including headers):\n",
    "\n",
    "```\n",
    "CulmenLength,CulmenDepth,FlipperLength,BodyMass\n",
    "39.1,18.7,181,3750\n",
    "49.1,14.8,220,5150\n",
    "46.6,17.8,193,3800\n",
    "```\n",
    "\n",
    "5. Connect the new **Enter Data Manually** module to the same **Dataset** input of the **Apply Transformation** module as the **Web Service Input**.\n",
    "\n",
    "6. The inference pipeline includes the **Evaluate Model** module, which is not useful when predicting from new data, so delete this module.\n",
    "\n",
    "7. Verify that your pipeline looks similar to the following:\n",
    "\n",
    "\n",
    "<img src=\"images/inference-clusters.png\"  />\n",
    "\n",
    "8. Submit the pipeline as a new experiment named **mslearn-penguin-inference** on your compute cluster. This may take a while!\n",
    "9. When the pipeline has finished, visualize the **Results dataset** output of the **Assign Data to Clusters** module to see the predicted cluster assignments and metrics for the three penguin observations in the input data.\n",
    "\n",
    "Your inference pipeline assigns penguin observations to clusters based on their features. Now you're ready to publish the pipeline so that client applications can use it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr></hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-KNmJVWySRQ"
   },
   "source": [
    "**<center><h1>Deploy a predictive service</h1></center>**\n",
    "\n",
    "After you've created and tested an inference pipeline for real-time inferencing, you can publish it as a service for client applications to use.\n",
    "\n",
    "<mark>Note:In this exercise, you'll deploy the web service to an Azure Container Instance (ACI). This type of compute is created dynamically, and is useful for development and testing. For production, you should create an inference cluster, which provide an Azure Kubernetes Service (AKS) cluster that provides better scalability and security.</mark>\n",
    "\n",
    "**<h2>Deploy a service</h2>**\n",
    "\n",
    "1. View the **Predict Penguin Clusters ** inference pipeline you created in the previous unit.\n",
    "2. At the top right, select **Deploy**, and deploy a new real-time endpoint, using the following settings:\n",
    "  - **Name:** predict-penguin-clusters\n",
    "  - **Description:** Cluster penguins.\n",
    "  - **Compute type:** Azure Container Instance\n",
    "3. Wait for the web service to be deployed - this can take several minutes. The deployment status is shown at the top left of the designer interface.\n",
    "\n",
    "\n",
    "**<h3>Test the service</h3>**\n",
    "\n",
    "Now you can test your deployed service from a client application - in this case, you'll use the code in the cell below to simulate a client application.\n",
    "\n",
    "1. On the **Endpoints page**, open the **predict-penguin-clusters** real-time endpoint.\n",
    "\n",
    "2. When the **predict-penguin-clusters** endpoint opens, view the **Consume** tab and note the following information there. You need this to connect to your deployed service from a client application.\n",
    "\n",
    " - The REST endpoint for your service\n",
    " - the Primary Key for your service\n",
    "\n",
    "3. Note that you can use the ⧉ link next to these values to copy them to the clipboard.\n",
    "\n",
    "4. With the **Consume** page for the **predict-penguin-clusters** service page open in your browser, open a new browser tab and open a second instance of Azure Machine Learning studio. Then in the new tab, view the **Notebooks** page (under **Author**).\n",
    "\n",
    "5. In the **Notebooks** page, under **My files**, use the 🗋 button to create a new file with the following settings:\n",
    "\n",
    "  - **File location:** Users/your user name\n",
    " - **File name:** Test-Diabetes.ipynb\n",
    " - **File type:** Notebook\n",
    " - **Overwrite if already exists:** Selected\n",
    "6. When the new notebook has been created, ensure that the compute instance you created previously is selected in the **Compute** box, and that it has a status of **Running**.\n",
    "\n",
    "7. Use the ≪ button to collapse the file explorer pane and give you more room to focus on the **Test-Penguins.ipynb** notebook tab.\n",
    "\n",
    "8. In the rectangular cell that has been created in the notebook, paste the following code:\n",
    "\n",
    "```\n",
    "endpoint = 'YOUR_ENDPOINT' #Replace with your endpoint\n",
    "key = 'YOUR_KEY' #Replace with your key\n",
    "\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "    \"Inputs\": {\n",
    "        \"WebServiceInput0\":\n",
    "        [\n",
    "            {\n",
    "                    'CulmenLength': 49.1,\n",
    "                    'CulmenDepth': 4.8,\n",
    "                    'FlipperLength': 1220,\n",
    "                    'BodyMass': 5150,\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    \"GlobalParameters\":  {\n",
    "    }\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ key)}\n",
    "\n",
    "req = urllib.request.Request(endpoint, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "    result = response.read()\n",
    "    json_result = json.loads(result)\n",
    "    output = json_result[\"Results\"][\"WebServiceOutput0\"][0]\n",
    "    print('Cluster: {}'.format(output[\"Assignments\"]))\n",
    "\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers to help debug\n",
    "    print(error.info())\n",
    "    print(json.loads(error.read().decode(\"utf8\", 'ignore')))\n",
    "```\n",
    "\n",
    "<mark>Note: Don't worry too much about the details of the code. It just defines features for a penguin, and uses the predict-penguin-clusters service you created to predict a cluster assignment.</mark>\n",
    "\n",
    "9. Switch to the browser tab containing the **Consume** page for the **predict-penguin-clusters** service, and copy the REST endpoint for your service. The switch back to the tab containing the notebook and paste the key into the code, replacing YOUR_ENDPOINT.\n",
    "\n",
    "10. Switch to the browser tab containing the **Consume** page for the **predict-penguin-clusters** service, and copy the Primary Key for your service. The switch back to the tab containing the notebook and paste the key into the code, replacing YOUR_KEY.\n",
    "\n",
    "11. Save the notebook. Then use the ▷ button next to the cell to run the code.\n",
    "\n",
    "12. Verify that predicted cluster is returned.\n",
    "\n",
    "<hr></hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx3mCr_dzgz9"
   },
   "source": [
    "**<center><h1>Summary</h1></center>**\n",
    "\n",
    "In this module, you learned how to use Azure Machine Learning designer to train and publish a classification model.\n",
    "\n",
    "**<h2>Clean-up</h2>**\n",
    "\n",
    "The web service you created is hosted in an Azure Container Instance. If you don't intend to experiment with it further, you should delete the endpoint to avoid accruing unnecessary Azure usage. You should also stop the compute instance until you need it again.\n",
    "\n",
    "1. In Azure Machine Learning studio, on the **Endpoints** tab, select the **predict-diabetes** endpoint. Then select **Delete** (🗑) and confirm that you want to delete the endpoint.\n",
    "2. On the **Compute** page, on the **Compute Instances** tab, select your compute instance and then select **Stop**.\n",
    "\n",
    "<mark>Note:</mark>\n",
    "\n",
    "<mark>Stopping your compute ensures your subscription won't be charged for compute resources. You will however be charged a small amount for data storage as long as the Azure Machine Learning workspace exists in your subscription. If you have finished exploring Azure Machine Learning, you can delete the Azure Machine Learning workspace and associated resources. However, if you plan to complete any other labs in this series, you will need to recreate it.</mark>\n",
    "\n",
    "<mark>To delete your workspace:</mark>\n",
    "\n",
    "<mark>1. In the Azure portal, in the Resource groups page, open the resource group you specified when creating your Azure Machine Learning workspace.</mark>\n",
    "<mark>2. Click Delete resource group, type the resource group name to confirm you want to delete it, and select Delete.</mark>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course2_Module4CreateAClusteriingModelWithAzureMachineLearningDesigner",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
